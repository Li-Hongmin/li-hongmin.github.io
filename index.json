[{"authors":["Li Hongmin"],"categories":null,"content":"Li Hongmin is a PhD student of Computer Science at Mathematical Modeling \u0026amp; Algorithms Laboratory in University of Tsukuba. His research interests include Spectral clustering, Machine learning, Big Data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Li Hongmin is a PhD student of Computer Science at Mathematical Modeling \u0026amp; Algorithms Laboratory in University of Tsukuba. His research interests include Spectral clustering, Machine learning, Big Data.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"","date":1602666665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602666665,"objectID":"e673ded6d076269015efd539757817ae","permalink":"/project/research_paper/","publishdate":"2020-10-14T18:11:05+09:00","relpermalink":"/project/research_paper/","section":"project","summary":"My papers","tags":["RP"],"title":"Research Papers","type":"project"},{"authors":["Hongmin Li","Xiuca Ye","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1602666364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602666364,"objectID":"c80cc156bb8a48a9dfdbfd5d83b812b3","permalink":"/publication/elsc/","publishdate":"2020-10-14T18:06:04+09:00","relpermalink":"/publication/elsc/","section":"publication","summary":"","tags":[],"title":"[ICMD 2020] Ensemble Learning for Spectral Clustering","type":"publication"},{"authors":["Hongmin Li","Xiucai Ye","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1602665880,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602665880,"objectID":"4b38c8b531aa9fd5e1f8d3bd81eb6fa8","permalink":"/publication/hnsc/","publishdate":"2020-07-19T17:58:00+09:00","relpermalink":"/publication/hnsc/","section":"publication","summary":"An advanced Nyström spectral clustering methods based on Hubness.","tags":["Sampling methods","Matrix decomposition","Clustering methods","Sparse matrices","Approximation error","Mathematical model","Computational complexity"],"title":"[IJCANN 2020] Hubness-based Sampling Method for Nystrom Spectral Clustering ","type":"publication"},{"authors":["Hongmin Li"],"categories":["tools"],"content":"Toolbox list   A matlab project template for clustering research  ","date":1593514681,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593514681,"objectID":"bc0fc37708e668f92f9f0be5fd0801e3","permalink":"/project/toolbox/","publishdate":"2020-06-30T19:58:01+09:00","relpermalink":"/project/toolbox/","section":"project","summary":"Made by myself","tags":["toolbox"],"title":"My Toolbox","type":"project"},{"authors":[],"categories":[],"content":"A matlab project template for clustering research After I finished a research paper on clustering, I found the project management is very important to ensure that the project can smoothly go.\nThat\u0026rsquo;s why I need a project template so that I can repeatly use it to conduct experiments in a short period.\n Here is a mindmap of this project. I create a clear structure to manage source code and the results.\nIn additon, the project can automatically generate latex tables to show the results.\nThe todolist is very usefull during conducting experiments, which lets you make sure which step you are working for.\nThe githus link is https://github.com/Li-Hongmin/Matlab_Project_Template_For_Research_Paper.git\n","date":1593513235,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593513235,"objectID":"9985395ded843645a6badcfb1e25ca55","permalink":"/post/200630_matlab_templete_clustering_project/","publishdate":"2020-06-30T19:33:55+09:00","relpermalink":"/post/200630_matlab_templete_clustering_project/","section":"post","summary":"A matlab project template for clustering research After I finished a research paper on clustering, I found the project management is very important to ensure that the project can smoothly go.\nThat\u0026rsquo;s why I need a project template so that I can repeatly use it to conduct experiments in a short period.\n Here is a mindmap of this project. I create a clear structure to manage source code and the results.","tags":["MATLAB","github","clustering"],"title":"A matlab project template for clustering research","type":"post"},{"authors":[],"categories":[],"content":"You can find this paper with its doi.\nLet\u0026rsquo;s try to read the abstract together.\nAbstract Definitions of Ensemble Clustering (EC) Ensemble Clustering (EC) aims to integrate multiple Basic Partitions (BPs) of the same dataset into a consensus one. It could be transformed as a graph partition problem on the co-association matrix derived from BPs.\nProblem Existing EC methods usually directly use the co-association matrix, yet without considering various noises (e.g., the disagreement between different BPs or outliers) that may exist in it. These noises can impair the cluster structure of a co-association matrix and thus degrade the final clustering performance.\nWell, you almost can say everything without considering the noises.\nMethod : Robust Spectral Ensemble Clustering (RSEC)  RSEC learns a robust representation for the co-association matrix through low-rank constraint, which reveals the cluster structure of a co-association matrix and captures various noises in it. RSEC finds the consensus partition by conducting spectral clustering.  These two steps are iteratively performed in a unified optimization framework.\nFigure 1: An illustration of the proposed $RSEC$, which simultaneously learns a low-rank representation $Z$ for the co-association matrix $S$ and finds the consensus partition $H$ by conducting spectral clustering on $L_z$. We employ $Z$ to reveal the cluster structure of $S$, and capture the noises inside $S$ with a sparse error matrix $E$. During the learning process, we use $H$ to iteratively enhance the block-diagonal structure of $Z$. The final clustering result could be obtained either from H or $Z$.\nOther details: Most importantly, during our optimization process, we utilize consensus partition to iteratively enhance the block-diagonal structure of the learned representation to further assist the clustering process. Experiments on numerous realworld datasets demonstrate the effectiveness of our method compared with the state-of-the-art. Moreover, several impact factors that may affect the clustering performance of our approach are also explored extensively.\n1. Somethings in Introduction: The contributions of this work are summarized as three-folds:\n  A unified optimization framework is provided to simultaneously learn a robust representation for the co-association matrix and find the final consensus partition.\n  The block-diagonal structure of the learned representation is iteratively enhanced by a consensus partition during the optimization process, thus it better uncovers the cluster structure of the co-association matrix.\n  But, if the order is random, block-diagonal structure may not be found, right?\nExperiments conducted on twelve real-wold datasets demonstrate the effectiveness of our approach over the state-of-the-art EC methods. Moreover, several impact factors that may affect the clustering performance of RSEC are also explored extensively.  2. Somethings in related work 2.1 Ensemble Clustering The first category employs a utility function to measure the similarity between the consensus clustering and multiple BPs, and usually finds the final partition by maximizing an explicit objective function.\n For instance,   For instance, Topchy et al.[27] proposed a Quadratic Mutual Information based objective function for consensus clustering, and used K-means clustering to find the solution. They further extended their work to using the EM algorithm with a finite mixture of multinomial distributions for consensus clustering [28]. Along this line, Wu et al.[32] transferred the ensemble clustering into a K-means clustering problem with KCC utility function and gave the necessary and sufficient conditions for KCC utility functions. In addition, there are some other interesting objective functions for the ensemble clustering, such as the ones based on nonnegative matrix factorization [12], kernel-based methods [30], and simulated annealing [18], respectively.\n The second category summarizes the information of input BPs into a co-association matrix, which counts how many times two instances occur in the same cluster. The co-association matrix ac- tually represents the pairwise similarity of all the data points in the partition space. Thus, a graph partition algorithm can be conducted on it to obtain the final clustering result.\n For instance,   Strehl and Ghosh [26] developed three graph-based algorithms for consensus clustering, while Fred and Jain [8] applied the agglomerative hierarchical clus- tering. Recently, Liu et al.[16] proposed a spectral ensemble clus- tering method, which ran spectral clustering on the co-association matrix and transformed it as a weighted K-means problem to achieve high efficiency. Other methods include Relabeling and Voting [1], Locally Adaptive Cluster based methods [6], genetic algorithm based methods [38], and still many more.\n 2.2 Low-Rank Matrix Analysis LRR [15, 14] assumes the data are drawn from a union of multiple low-dimensional subspaces, and tries to recover these subspaces by seeking the lowest rank representation Z for X as:\n $$ \\min _{\\mathbf{Z}, \\mathbf{E}} \\operatorname{rank}(\\mathbf{Z})+\\lambda\\|\\mathbf{E}\\|_{0} \\text { s.t.} \\mathbf{X}=\\mathbf{X} \\mathbf{Z}+\\mathbf{E} $$  where λ  0 balances the rankness of Z and the sparseness of the error matrix E. Since Eq.(1) is a NP-hard problem, we usually solve its convex relaxation by using nuclear norm to estimate the rank(Z) and l1 or l2, 1 norm to approximate ∥E∥0. Actually, since the minimizer of Eq.(1) also obtains a low-rank matrix XZ for X, LRR could be seen as a generalization for the Robust PCA [14]. It is worthy to note that, by utilizing X to express itself, Z is actu- ally a similarity matrix that reveals the membership between data points. Moreover, it has been proven that Z enjoys a nice block- diagonal property [14, 37], which can uncover the global structure of data and further facilitate the clustering task. Therefore, in this paper, we employ LRR to learn a robust representation for the co- association matrix, rather than directly recover it as a low-rank one.\nEmmy, I guess this is important for this paper. I\u0026rsquo;m not very clear what author said. It seems works.\n3. ROBUST SPECTRAL ENSEMBLE CLUSTERING Let X = {x1, x2, ··· , xn} be a set of n data points independently sampled from K clusters, represented as C = {C1 , · · · , Ck }.\nDenote Π = {π1, π2, ··· , πr} as r input basic partitions (BPs), each of which divides X into Ki crisply partitions and maps X into a label set πi = {πi(x1), πi(x2), · · · , πi(xn)}, where Ki is the cluster number for the i-th BP, 1 ≤ πi(xj) ≤ Ki, and 1 ≤ i ≤ r, 1 ≤ j ≤ n.\nNote that, the cluster number of each BP is usually set to be different for achieving the diversity among input BPs, which has been recognized as an efficient manner to ensure the success for ensemble clustering [1, 32].\nAgree\nThe goal of ensemble clustering is to find the consensus parti- tion that agrees with input BPs most and divides X into its origi- nal K clusters. Commonly, EC methods may summarize r BPs as a co-association matrix, and then conduct a graph partition algo- rithm to obtain the final consensus clustering, denoted as π. The co-association matrix S ∈ Rn×n actually calculates the times of two instances occurring in the same cluster based on Π, which is defined as [8]:\n $$ \\mathbf{S}\\left(x_{p}, x_{q}\\right)=\\sum_{i=1}^{r} \\delta\\left(\\pi_{i}\\left(x_{p}\\right), \\pi_{i}\\left(x_{q}\\right)\\right) $$  where $x_{p}, x_{q} \\in \\mathcal{X}$ and δ(a, b) is 1 if a = b; 0 otherwise. Obvi- ously, S could be normalized by S = S/r. Inspired by [16], we apply the spectral clustering on the co-association matrix S, and have its trace minimization form by following [19] as\n $$ \\min _{\\mathbf{H}} \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{s} \\mathbf{H}\\right) \\text { s.t.} \\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}, $$   $$ \\mathbf{L}_{s}=\\mathbf{I}-\\mathbf{D}_{s}^{-1 / 2} \\mathbf{S} \\mathbf{D}_{s}^{-1 / 2} $$  of S, with degree matrix $D_s$ ∈ Rn×n being a diagonal matrix whose jth diagonal element is the sum of the jth row of S, and H ∈ Rn×K is defined as the scaled partition matrix of π:\n $$ \\mathbf{H}_{j k}=\\left\\{\\begin{array}{ll}{1 / \\sqrt{\\left|C_{k}\\right|}, } \u0026 {\\text { if } x_{j} \\in C_{k} \\text { in } \\pi} \\\\ {0, } \u0026 {\\text { otherwise }}\\end{array}\\right. $$  We can use H to represent the final ensemble clustering result.\nReally? interesting, this is first time I read this.\n3.2 Problem Formulation Given a normalized co-association matrix S, the objective func- tion of our RSEC is formulated as:\n $$ \\begin{array}{l}{\\min _{\\mathbf{H}, \\mathbf{z}, \\mathbf{E}} \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{z} \\mathbf{H}\\right)+\\lambda_{1}\\|\\mathbf{Z}\\|_{*}+\\lambda_{2}\\|\\mathbf{E}\\|_{2, 1}} \\\\ {\\text { s.t.} \\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}, \\mathbf{S}=\\mathbf{S} \\mathbf{Z}+\\mathbf{E}}\\end{array} $$  with $$ \\mathbf{L}_{z}=\\mathbf{I}-\\mathbf{D}_{z}^{-1 / 2}\\left(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2+\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\right) \\mathbf{D}_{z}^{-1 / 2} $$ where \\(\\mathbf{H}\\) denotes the consensus partition, \\(\\mathbf{Z} \\in \\mathbb{R}^{n \\times n}\\) is the learned representation, \\(\\mathbf{E} \\in \\mathbb{R}^{n \\times n}\\) is an error matrix that tries to capture various noises inside \\(\\mathbf{S}, \\) and \\(\\lambda_{1}, \\lambda_{2}0\\) are two penalty parameters to balance the corresponding terms. In Eq.(5), \\(\\mathbf{L}_{z}\\) is designed as a normalized Laplacian matrix of the graph constructed by \\(\\mathbf{Z}\\) and \\(\\mathbf{H}, \\) and the degree matrix \\(\\mathbf{D}_{z}\\) is computed by:   $$ \\mathbf{L}_{z}=\\mathbf{I}-\\mathbf{D}_{z}^{-1 / 2}\\left(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2+\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\right) \\mathbf{D}_{z}^{-1 / 2} $$   where \\(d_{j}, 1 \\leq j \\leq n, \\) is the sum of the \\(j\\) -th row of the matrix \\(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2+\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\). Here, we employ \\(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2\\) instead of \\(\\mathbf{Z}\\) to achieve a symmetric graph. Moreover, since \\(\\mathbf{H}\\) is a high-quality clustering result, \\(\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\) enjoys a clear cluster structure. Thus, we use it to further enhance the block-diagonal structure of \\(\\mathbf{Z}\\).  3.3 Optimization : The Augmented Lagrange Multiplier (ALM) The Augmented Lagrange Multiplier (ALM) method [13] comes to mind as an efficient and effective solver to our problem. To apply ALM, we first introduce an auxiliary variable J to make Eq.(5) separable, and equivalently convert it as:\n $$ \\begin{array}{ll}{} \u0026 {\\min _{\\mathbf{H}, \\mathbf{Z}, \\mathbf{E}} \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{z} \\mathbf{H}\\right)+\\lambda_{1}\\|\\mathbf{J}\\|_{*}+\\lambda_{2}\\|\\mathbf{E}\\|_{2, 1}} \\\\ {} \u0026 {\\text { s.t.} \\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}, \\mathbf{S}=\\mathbf{S} \\mathbf{Z}+\\mathbf{E}, \\mathbf{Z}=\\mathbf{J}}\\end{array} $$   Following \\([5], \\) the constraint \\(\\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}\\) is relaxed to avoid hard partition during the optimization process. Then, the augmented La- grangian function of Eq.( 8) is:   \\(\\begin{aligned} \\mathcal{L}=\u0026 \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{z} \\mathbf{H}\\right)+\\lambda_{1}\\|\\mathbf{J}\\|_{*}+\\lambda_{2}\\|\\mathbf{E}\\|_{2, 1} \\\\ \u0026+\\left\\langle\\mathbf{Y}_{1}, \\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}\\right\\rangle+\\left\\langle\\mathbf{Y}_{2}, \\mathbf{Z}-\\mathbf{J}\\right\\rangle \\\\ \u0026+\\frac{\\mu}{2}\\left(\\|\\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Z}-\\mathbf{J}\\|_{\\mathrm{F}}^{2}\\right) \\end{aligned}\\)  where Y1 and Y1 are two Lagrangian multipliers, and μ \u0026gt; 0 is the penalty parameter.\nThe ALM solver solves Eq.(9) with an iterative update manner, which addresses J, Z, E, and H in sequence and optimizes one variable at a time by fixing the others. More details are given in the following.\nUpdata J.  We first minimize (\\mathcal{L}) with respect to (\\mathrm{J}, ) and obtain (\\mathrm{J}^{(t+1)}) by: $$ \\begin{aligned} \\quad \u0026amp; \\text { argmin } \\lambda_{1}|\\mathbf{J}|_{*}+\\left\\langle\\mathbf{Y}_{2}^{(t)}, \\mathbf{Z}^{(t)}-\\mathbf{J}\\right\\rangle+\\frac{\\mu^{(t)}}{2}\\left|\\mathbf{Z}^{(t)}-\\mathbf{J}\\right|_{\\mathrm{F}}^{2} \\\n= \\underset{\\mathbf{J}}{\\operatorname{argmin}} \\frac{\\lambda_{1}}{\\mu^{(t)}}|\\mathbf{J}|_{*}+\\frac{1}{2}\\left|\\mathbf{J}-\\left(\\mathbf{Z}^{(t)}+\\frac{1}{\\mu^{(t)}} \\mathbf{Y}_{2}^{(t)}\\right)\\right|_{\\mathrm{F}}^{2} \\end{aligned} $$\n Eq.(10) could be solved by the Singular Value Thresholding (SVT) operator [3], which has a closed-form solution as:\n $$ \\begin{array}{c}{\\mathbf{J}^{(t+1)}=\\Theta_{\\frac{\\lambda_{1}}{\\mu^{(t)}}}\\left(\\mathbf{Z}^{(t)}+\\frac{1}{\\mu^{(t)}} \\mathbf{Y}_{2}^{(t)}\\right)} \\\\ {\\text { where } \\Theta(\\cdot) \\text { is the SVT operator.}}\\end{array} $$  Update Z  Update \\(\\mathbf{Z} .\\) By substituting Eq.(6) into \\(\\mathcal{L}\\) and dropping unrelated terms, the subproblem for updating \\(\\mathbf{Z}\\) is equivalent to the following: \\\\ argmin \\(-\\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{H}^{(t) T} \\mathbf{D}_{z}^{-1 / 2}\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) \\mathbf{D}_{z}^{-1 / 2} \\mathbf{H}^{(t)}\\right)\\) \\(\\quad+\\left(\\mathbf{Y}_{1}^{(t)}, \\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}^{(t)}\\right\\rangle+\\left\\langle\\mathbf{Y}_{2}^{(t)}, \\mathbf{Z}-\\mathbf{J}^{(t+1)}\\right\\rangle\\) \\(\\quad+\\frac{\\mu^{(t)}}{2}\\left(\\left\\|\\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}^{(t)}\\right\\|_{\\mathbf{F}}^{2}+\\left\\|\\mathbf{Z}-\\mathbf{J}^{(t+1)}\\right\\|_{\\mathbf{F}}^{2}\\right)\\)   Note that, the derivative of \\(\\mathbf{D}_{z}\\) with respect to \\(\\mathbf{Z}\\) is relatively complex, which actually complicates the solution of obtaining \\(\\mathbf{Z}^{(t+1)} .\\)   By fixing \\(D_{z}, \\) Eq.( 12) becomes a quadratic problem of \\(\\mathbf{Z} .\\) Thus, taking the derivative of \\(\\mathcal{L}\\) with respect to \\(\\mathbf{Z}\\) gives \\(\\mathbf{Z}^{(t+1)}\\) as: \\(\\mathbf{Z}^{(t+1)}=\\left(\\mathbf{S S}^{\\mathrm{T}}+\\mathbf{I}\\right)^{-1}\\left(\\mathbf{S}^{\\mathrm{T}} \\mathbf{S}+\\mathbf{J}^{(t+1)}-\\mathbf{S}^{\\mathrm{T}} \\mathbf{E}^{(t)}+\\right.\\) \\(\\left.\\quad \\frac{1}{\\mu^{(t)}}\\left(\\mathbf{S}^{\\mathrm{T}} \\mathbf{Y}_{1}^{(t)}-\\mathbf{Y}_{2}^{(t)}+\\mathbf{D}_{z}^{-1 / 2} \\mathbf{H}^{(t)} \\mathbf{H}^{(t) \\mathrm{T}} \\mathbf{D}_{z}^{-1 / 2}\\right)\\right)\\)  Update E. Update Multipliers ","date":1577436212,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577436212,"objectID":"13b7e9b08aeaea23632a3b077a51ddab","permalink":"/post/lr2_rsec/","publishdate":"2019-12-27T17:43:32+09:00","relpermalink":"/post/lr2_rsec/","section":"post","summary":"You can find this paper with its doi.\nLet\u0026rsquo;s try to read the abstract together.\nAbstract Definitions of Ensemble Clustering (EC) Ensemble Clustering (EC) aims to integrate multiple Basic Partitions (BPs) of the same dataset into a consensus one. It could be transformed as a graph partition problem on the co-association matrix derived from BPs.\nProblem Existing EC methods usually directly use the co-association matrix, yet without considering various noises (e.","tags":["Ensemble Clustering; Spectral Clustering; Co-association Matrix; Low-rank Representation"],"title":"Literature Review #2: Robust Spectral Ensemble Clustering","type":"post"},{"authors":[],"categories":[],"content":"This excellent paper was punished by Ana L. N. Fred and Anil K. Jain in 2005 IEEE Transactions on Pattern Analysis and Machine Intelligence. It proposed An clustering ensemble is based on a voting strategy for various partitions. Outline of evidence accumulation clustering algorithm is below: 1. Introduction Here are literature review. All methods are old enough. I guess we can skim it.\nA list of clustering application  A number of application areas use clustering techniques for organizing or discovering structure in data, such as data mining [1], [2], information retrieval [3], [4], [5], image segmentation [6], and machine learning.\n A list of clustering methods  Examples of model-based techniques include:\n parametric density approaches, such as mixture decomposition techniques [23], [24], [25], [26]; prototype-based methods, such as central clustering [14], square-error clustering [27], K-means [28], [8], or K-medoids clustering [9]; and shape fitting approaches [15], [6], [16].  Most of the above techniques utilize an optimization procedure tuned to a particular cluster shape, or emphasize cluster compactness.\n Fisher et al. [31] proposed an optimization-based clustering algorithm, based on a pairwise clustering cost function, emphasizing cluster connectedness.\n  Nonparametric density-based clustering methods attempt to identify high density clusters separated by low density regions [5], [32], [33].\n  Graph-theoretical approaches [34] have mostly been explored in hierarchical methods that can be represented graphically as a tree or dendrogram [7], [8]. Both agglomerative [28], [35] and divisive approaches [36] (such as those based on the minimum spanning tree—MST [28]) have been proposed; different algorithms are obtained depending on the definition of similarity measures between patterns and between clusters [37]. The single-link (SL) and the complete-link (CL) hierarchical methods [7], [8] are the best known techniques in this class, emphasizing, respectively, connectedness and compactness of patterns in a cluster. Prototype-based hierarchical methods, which define similarity between clusters based on cluster representatives, such as the centroid, emphasize compactness. Variations of the prototype-based hierarchical clustering include the use of multiple prototypes per cluster, as in the CURE algorithm [38]. Other hierarchical agglomerative clustering algorithms follow a split and merge technique, the data being initially split into a large number of small clusters, merging being based on intercluster similarity; a final partition is selected among the clustering hierarchy by thresholding techniques are based or measures of cluster validity [39], [5], [40], [41], [42], [43].\n  Treating the clustering problem as a graph partitioning problem, a recent approach, known as spectral clustering, applies spectral graph theory for clustering [44], [45], [46].\n   The characteristic of K-means algorithm   minimizes the squared-error criteria, is one of the simplest clustering algorithm.\n  It is computationally efficient and does not require the user to specify many parameters.\n  Its major limitation, however, is the inability to identify clusters with arbitrary shapes, ultimately imposing hyperspherical shaped clusters on the data.\n  Extensions of the basic K-means algorithm include: use of Mahalanobis distance to identify hyperellipsoidal clusters [28], introducing fuzzy set theory to obtain nonexclusive partitions [20], and adaptations to straight line fitting [47].\n  Problem description While hundreds of clustering algorithms exist, it is difficult to find a single clustering algorithm that can handle all types of cluster shapes and sizes or even decide which algorithm would be the best one for a particular data set [48], [49].\nFigure 1 Results of clusterings using different algorithms (K-means, single-link—SL, and complete-link—CL) with different parameters. Each cluster identified is shown in a different color/pattern.(a) Input data.(b) K-means clustering, k=8.(c) Clustering with the SL method, threshold at 0.55, resulting in 27 clusters.(d) Clustering with the SL method, forcing eight clusters.(e) Clustering with the CL method, threshold at 2.6, resulting in 22 clusters.(f) Clustering with the CL method, forcing eight clusters.\n Related Work and why use k-means as basic partition method   Inspired by the work in sensor fusion and classifier combination [50], [51], [52], a clustering combination approach has been proposed [53], [54], [55]. Fred and Jain introduce the concept of evidence accumulation clustering that maps the individual data partitions in a clustering ensemble into a new similarity measure between patterns, summarizing interpattern structure perceived from these clusterings. The final data partition is obtained by applying the single-link method to this new similarity matrix. The results of this method show that, the combination of “weak” clustering algorithms such as the K-means, which impose a simple structure on the data, can lead to the identification of true underlying clusters with arbitrary shapes, sizes and densities. Strehl and Ghosh [56] explore the concept of consensus between data partitions and propose three different combination mechanisms. The first step of the consensus functions is to transform the data partitions into a hypergraph representation. The hypergraph-partitioning algorithm (HGPA) obtains the combined partition by partitioning the hypergraph into k unconnected components of approximately the same size, by cutting a minimum number of hyperedges. The metaclustering algorithm (MCLA) applies a graph-based clustering to hyperedges in the hypergraph representation. Finally, CSPA uses a pairwise similarity, as defined by Fred and Jain [55], and the final data partition is obtained by applying the METIS algorithm of Karypis and Kumar to the induced similarity measure between patterns.\n 2. Problem Formulation Consider $N$ partitions of the data $X$ and let $(\\mathrm{F})$ represent the set of $N$ partitions, which we define as a clustering ensemble:\n$$\\mathrm{P}=\\left\\{P^{1}, P^{2}, \\ldots, P^{N}\\right\\}$$  $$ \\begin{aligned} P^{1} \u0026=\\left\\{C_{1}^{1}, C_{2}^{1}, \\ldots, C_{k_{1}}^{1}\\right\\} \\\\ \u0026 \\vdots \\\\ P^{N} \u0026=\\left\\{C_{1}^{N}, C_{2}^{N}, \\ldots, C_{k_{N}}^{N}\\right\\} \\end{aligned} $$  where $C_{j}^{i}$ is the $j$ th cluster in data partition $P^{i},$ which has $k_{i}$ clusters and $n_{j}^{i}$ is the cardinality of $C_{j}^{i},$ with $\\sum_{j=1}^{k_{i}} n_{j}^{i}=n, \\quad i=1, \\ldots, N$\nThe problem is to find an \u0026ldquo;optimal\u0026rdquo; data partition, $P^{},$ using the information available in $N$ different data partitions in $\\mathrm{FP}=\\left{P^{1}, P^{2}, \\ldots, P^{N}\\right} .$ We define $k^{}$ as the number of clusters in $P^{} .$ Ideally, $P^{}$ should satisfy the following properties:\n Consistency with the clustering ensemble $\\mathrm{F}$; Robustness to small variations in $\\mathrm{F}$; Goodness of fit with ground truth information (true cluster labels of patterns), if available.  3. Evidence accumulation clustering 3.1 Producing Clustering Ensembles Clustering ensembles can be generated by following two approaches:\n1) choice of data representation In the first approach, different partitions of the objects under analysis may be produced by:\na) employing different preprocessing and/or feature extraction mechanisms, which ultimately lead to different pattern representations (vectors, strings, graphs, etc.) or different feature spaces\nb) exploring subspaces of the same data representation, such as using subsets of features\nc) perturbing the data, such as in bootstrapping techniques (like bagging), or sampling approaches, as, for instance, using a set of prototype samples to represent huge data sets.\n 2) choice of clustering algorithms or algorithmic parameters.   In the second approach, we can generate clustering ensembles by:\ni) applying different clustering algorithms,\nii) using the same clustering algorithm with different parameters or initializations,\niii) exploring different dissimilarity measures for evaluating interpattern relationships, within a given clustering algorithm.\n 3.2 Combining Evidence: The Co-Association Matrix Taking the co-occurrences of pairs of patterns in the same cluster as votes for their association, the $N$ data partitions of $n$ patterns are mapped into a $n \\times n$ co-association matrix: $$ \\mathcal{C}(i, j)=\\frac{n_{i j}}{N} $$\nwhere $n_{i j}$ is the number of times the pattern pair $(i, j)$ is assigned to the same cluster among the $N$ partitions.\nAuthors give an illustration of proposed methods in figure below:\nFig. 2. Individual clusterings and combination results on concentric clusters using the K-means algorithm. (a) Data set with concentric clusters. (b) First run of K-means, k=25. (c) Second run of K-means, k=11. (d) Plot of the interpattern similarity matrix for the data in (a). (e) Co-association matrix for the clustering in (b). (f) Co-association matrix for the clustering in (c). (g) Co-association matrix based on the combination of 30 clusterings. (h) Two-dimensional multidimensional scaling of the co-association matrix in (g). (i) Evidence accumulation data partition.\n3.3 Recovering Natural Clusters The core of the evidence accumulation clustering technique is the mapping of partitions into the co-association matrix, $C$. As figure shows, the clustering is obtained using the a MST-based clustering with co-association matrix.\nFig. 3. Dendrogram produced by the SL method using the similarity matrix in Fig. 2g. Distances (1−similarity) are represented along the graph ordinate. From the dendrogram, the following cluster lifetimes are identified: 2-clusters: l2=0.18, 3-clusters: l3=0.36, 4-clusters: l4=0.14, 5-clusters: 0.02. The 3-cluster partition (shown in Fig. 2i), corresponding to the longest lifetime, is chosen (threshold on the dendrogram is between 0.4 and 0.76).\n","date":1577171080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577171080,"objectID":"b5ce6df147e00c9e2a43d2a2dcf397eb","permalink":"/post/notepaper_combining_multiple_clustering_using_evidence_accumulation/","publishdate":"2019-12-24T16:04:40+09:00","relpermalink":"/post/notepaper_combining_multiple_clustering_using_evidence_accumulation/","section":"post","summary":"This excellent paper was punished by Ana L. N. Fred and Anil K. Jain in 2005 IEEE Transactions on Pattern Analysis and Machine Intelligence. It proposed An clustering ensemble is based on a voting strategy for various partitions. Outline of evidence accumulation clustering algorithm is below: 1. Introduction Here are literature review. All methods are old enough. I guess we can skim it.\nA list of clustering application  A number of application areas use clustering techniques for organizing or discovering structure in data, such as data mining [1], [2], information retrieval [3], [4], [5], image segmentation [6], and machine learning.","tags":["clustering ensemble","consensus clustering"],"title":"Literature Review #1: Combining multiple clustering using evidence accumulation ","type":"post"},{"authors":[],"categories":[],"content":"学习笔记集合：  介绍了简单的Hugo应用。  跳转到 Day1\n介绍了github pages和部署。  跳转到 Day2\n","date":1577031190,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577031190,"objectID":"30ea716f1df0833734efacb1bc50f758","permalink":"/project/hugo/hugo/","publishdate":"2019-12-23T01:13:10+09:00","relpermalink":"/project/hugo/hugo/","section":"project","summary":"A notebook for using hugo to create my website","tags":["hugo"],"title":"【学习笔记】Hugo 建站","type":"project"},{"authors":[],"categories":[],"content":"上次我们已经做个一个网站在本地了，这次要把它部署到Github pages上。\n本文主要参考 hugo官方文档,需要github和本地两别操作。\n部署前的准备 首先要确认三点：\n 确认本地有无 git:  git --version # git version 2.21.0 (Apple Git-122.2)   确认 GitHub账号有无，可以免费申请一个。\n  已经有一个可以发布的Hugo网站\n  两种GitHub Pages  User/Organization Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io/) Project Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io//)  部署设置   在github创建新的pages repository，如果是第一种就把\u0026lt;USERNAME|ORGANIZATION\u0026gt;换成你的用户名或者组织名，则输入如下图：   我因为已经有同名的repository了，所以不可以创建。     再用同样的方法在github创建新的repository用来放hugo的文件，以下用\u0026lt;YOUR-PROJECT\u0026gt;代替这个repository的名字\n  本地git clone，把\u0026lt;YOUR-PROJECT-URL\u0026gt;换成网页url地址，在合适的目录下执行命令: git clone \u0026lt;YOUR-PROJECT-URL\u0026gt; \u0026amp;\u0026amp; cd \u0026lt;YOUR-PROJECT\u0026gt;\n  复制粘贴之前hugo的文件到clone的目录下面。用hugo server来检查下正不正常，浏览器打开 http://localhost:1313。\n  测试过后，用rm -rf public/来移除所有public下的文件。\n  创建一个submodule：git submodule add -b master git@github.com:\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public\n  这个submodule就会同步到\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io，而整个本地目录会同步到\u0026lt;YOUR-PROJECT\u0026gt;这里。\n部署网站 生成网站并部署 hugo cd public git add . git commit -m \u0026quot;Build website\u0026quot; git push origin master cd ..  这样应就可以了，浏览器打开\u0026lt;USERNAME\u0026gt;.github.io就可以看到了。\n自动部署脚本 可以创建一个deploy.sh文件，好像 这样.\n然后记得chmod +x deploy.sh赋予权限。\n只要运行 ./deploy.sh \u0026quot;Your optional commit message\u0026quot;就可以提交更改了。\n总结 流水线作业，总是不够详细。\n另外本网站的制作是根据academic模版，依照其 官网而部署，也很简单。\n","date":1577026400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577026400,"objectID":"162c2b8665175b7eb48685befbbbd570","permalink":"/post/191222_hugo/","publishdate":"2019-12-22T23:53:20+09:00","relpermalink":"/post/191222_hugo/","section":"post","summary":"上次我们已经做个一个网站在本地了，这次要把它部署到Github pages上。\n本文主要参考 hugo官方文档,需要github和本地两别操作。\n部署前的准备 首先要确认三点：\n 确认本地有无 git:  git --version # git version 2.21.0 (Apple Git-122.2)   确认 GitHub账号有无，可以免费申请一个。\n  已经有一个可以发布的Hugo网站\n  两种GitHub Pages  User/Organization Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io/) Project Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io//)  部署设置   在github创建新的pages repository，如果是第一种就把\u0026lt;USERNAME|ORGANIZATION\u0026gt;换成你的用户名或者组织名，则输入如下图：   我因为已经有同名的repository了，所以不可以创建。     再用同样的方法在github创建新的repository用来放hugo的文件，以下用\u0026lt;YOUR-PROJECT\u0026gt;代替这个repository的名字\n  本地git clone，把\u0026lt;YOUR-PROJECT-URL\u0026gt;换成网页url地址，在合适的目录下执行命令: git clone \u0026lt;YOUR-PROJECT-URL\u0026gt; \u0026amp;\u0026amp; cd \u0026lt;YOUR-PROJECT\u0026gt;\n  复制粘贴之前hugo的文件到clone的目录下面。用hugo server来检查下正不正常，浏览器打开 http://localhost:1313。\n  测试过后，用rm -rf public/来移除所有public下的文件。","tags":["hugo","github pages","personal website"],"title":"【学习笔记】用Hugo建站来写个人博客 Day2","type":"post"},{"authors":[],"categories":[],"content":"建站一直是一个很复杂的工程，不过总有捷径。本文介绍一种快速建立静态网站的工具hugo。hugo是基于go语言新兴静态网站生成工具，非常快速轻便。我们只需要用markdown来写博客直接生成网页，可以让我们专注于内容。另外有超多主题可供选择，换个装潢模版也很容易。\n缘起 我目前在读Ph.D平时读论文时会做一些笔记，一直想把这些笔记整理分享出来。 我于是开始尝试使用个人博客来记录学习过程整理思路，之后还能做一些页面展示研究成果。\n通过一番考察我选择先用 Hugo建立一个网站。 详细Hugo是什么请读者自行了解，我理解就是go语言写的快捷建站工具，并且只要用markdown语言就可以写页面了。 同时在过程中把自己的心得分享出来供后来者参考。\n安装和一键建站 这里我主要参考官网的 Quick Start。\n 安装 Hugo 我的环境时macOS，使用Homebrew安装：  brew install hugo hugo version  最后得到这样的信息：Hugo Static Site Generator v0.61.0/extended darwin/amd64 BuildDate: unknown。\n另外说Homebrew是一个非常好的软件管理软件，一行代码就安装好了，如果还没有试过的人一定要尝试一下。\n如果是Windows或linux用户的话参考 这里进行安装。\n一键建站  在terminal或命令行里找个合适的文件路径，执行以下命令。\nhugo new site yourFolderName  这时Hugo会生成一个网站模版。命令中_yourFolderName_就是文件夹名，可以随便起。\n添加主题  主题是Hugo的一大优势，海量主题任你选。这里是 主题库。\n我们这里就用官方教程里的Ananke。\ncd quickstart # Download the theme git init git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke # Note for non-git users: # - If you do not have git installed, you can download the archive of the latest # version of this theme from: # https://github.com/budparr/gohugo-theme-ananke/archive/master.zip # - Extract that .zip file to get a \u0026quot;gohugo-theme-ananke-master\u0026quot; directory. # - Rename that directory to \u0026quot;ananke\u0026quot;, and move it into the \u0026quot;themes/\u0026quot; directory. # End of note for non-git users. # Edit your config.toml configuration file # and add the Ananke theme. echo 'theme = \u0026quot;ananke\u0026quot;' \u0026gt;\u0026gt; config.toml  添加页面  生成第一个页面。\nhugo new posts/my-first-post.md  第一个页面看起来像这样：\n--- title: \u0026quot;My First Post\u0026quot; date: 2019-03-26T08:47:11+01:00 draft: true ---  可以用你喜欢的IDE来写markdown文件，直接就可以生成网站了。\n运行Hugo服务器  终于到来一键生成网站的时候了，虽然这里的一键是一行命令。\nhugo server -D  最后提示\nWeb Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop  浏览器打开 http://localhost:1313/看看是不是成功了。\n看上去就像是 这样子。\n总结 简单易学，一行命令完事。\n建站大坑，容我慢慢来填。\n","date":1577025505,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577025505,"objectID":"0b0fe5b12b6b85ce0eecc58d978c183e","permalink":"/post/191217_hugo/","publishdate":"2019-12-22T23:38:25+09:00","relpermalink":"/post/191217_hugo/","section":"post","summary":"建站一直是一个很复杂的工程，不过总有捷径。本文介绍一种快速建立静态网站的工具hugo。hugo是基于go语言新兴静态网站生成工具，非常快速轻便。我们只需要用markdown来写博客直接生成网页，可以让我们专注于内容。另外有超多主题可供选择，换个装潢模版也很容易。\n缘起 我目前在读Ph.D平时读论文时会做一些笔记，一直想把这些笔记整理分享出来。 我于是开始尝试使用个人博客来记录学习过程整理思路，之后还能做一些页面展示研究成果。\n通过一番考察我选择先用 Hugo建立一个网站。 详细Hugo是什么请读者自行了解，我理解就是go语言写的快捷建站工具，并且只要用markdown语言就可以写页面了。 同时在过程中把自己的心得分享出来供后来者参考。\n安装和一键建站 这里我主要参考官网的 Quick Start。\n 安装 Hugo 我的环境时macOS，使用Homebrew安装：  brew install hugo hugo version  最后得到这样的信息：Hugo Static Site Generator v0.61.0/extended darwin/amd64 BuildDate: unknown。\n另外说Homebrew是一个非常好的软件管理软件，一行代码就安装好了，如果还没有试过的人一定要尝试一下。\n如果是Windows或linux用户的话参考 这里进行安装。\n一键建站  在terminal或命令行里找个合适的文件路径，执行以下命令。\nhugo new site yourFolderName  这时Hugo会生成一个网站模版。命令中_yourFolderName_就是文件夹名，可以随便起。\n添加主题  主题是Hugo的一大优势，海量主题任你选。这里是 主题库。\n我们这里就用官方教程里的Ananke。\ncd quickstart # Download the theme git init git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke # Note for non-git users: # - If you do not have git installed, you can download the archive of the latest # version of this theme from: # https://github.","tags":["hugo","blog","建站，博客"],"title":"【学习笔记】用Hugo建站来写个人博客 Day1","type":"post"},{"authors":["Xiucai Ye","Hongmin Li","Tetsuya Sakurai","Pei-Wei Shueng"],"categories":[],"content":"","date":1577007783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577007783,"objectID":"5200b2210c2d271d03c2eac782f8f257","permalink":"/publication/ensemble_feature_learning_to_identify_risk_factors_for_predicting_secondary_cancer/","publishdate":"2019-12-22T18:43:03+09:00","relpermalink":"/publication/ensemble_feature_learning_to_identify_risk_factors_for_predicting_secondary_cancer/","section":"publication","summary":"An effective ensemble feature learning method to identify the risk factors for predicting secondary cancer by considering class imbalance and patient heterogeneity.","tags":["secondary cancer","risk factors","class imbalance","patient heterogeneity","spectral clustering","ensemble learning"],"title":"Ensemble Feature Learning to Identify Risk Factors for Predicting Secondary Cancer","type":"publication"},{"authors":["Xiucai Ye","Hongmin Li","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1576898274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576898274,"objectID":"612195691a61badc10a9bf828817b090","permalink":"/publication/distributed_collaborative_feature_selection_based_on_intermediate_representation/","publishdate":"2019-08-10T12:17:54+09:00","relpermalink":"/publication/distributed_collaborative_feature_selection_based_on_intermediate_representation/","section":"publication","summary":"A novel distributed method which allows collaborative feature selection for multiple parties without revealing their original data.","tags":["Unsupervised Learning","Feature Selection","Learning Sparse Models","Dimensionality Reduction and Manifold Learning"],"title":"[IJCAI 2019] Distributed collaborative feature selection based on intermediate representation","type":"publication"},{"authors":["Xiucai Ye","Hongmin Li","Tetsuya Sakurai","Zhi Liu"],"categories":["Spectral Clustering"],"content":"","date":1576849824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576849824,"objectID":"4809cbd1424ea9165d0ab082220e5751","permalink":"/publication/lsch/","publishdate":"2018-10-08T00:00:00Z","relpermalink":"/publication/lsch/","section":"publication","summary":"An accelerated spectral clustering method based on sparse presentation where each data point is presented as sparse linear combinations of a part of representative data points.","tags":["Spectral Clustering","Hubness","Large Scale","Sparse Representation"],"title":"Large Scale Spectral Clustering Using Sparse Representation Based on Hubness","type":"publication"}]