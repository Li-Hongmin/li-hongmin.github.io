[{"authors":["Li Hongmin"],"categories":null,"content":"Li Hongmin is a PhD student of Computer Science at Mathematical Modeling \u0026amp; Algorithms Laboratory in University of Tsukuba. His research interests include Spectral clustering, Machine learning, Big Data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://li-hongmin.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Li Hongmin is a PhD student of Computer Science at Mathematical Modeling \u0026amp; Algorithms Laboratory in University of Tsukuba. His research interests include Spectral clustering, Machine learning, Big Data.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"前言，这是写给研究室同学的一个教学文章。目的是让大家快速学会很多我亲身验证好用的工具，以便大家日后在研究和工作时能更加游刃有余。 另外由于时间和精力有限，这里所有的内容，或者工具推荐的具体使用方法请自行上网搜索。\n操作系统OS 大家都有mac电脑了，如果之前没用过的话，还得适应一下可能。\n从Windows转到macOS   要从Windows转到macOS吗?\n 随便。不过就算你不用macOS来工作，你可以装一个chrome浏览器，在macOS下看网页，聊微信，做休闲的事。这样的好处是，当有一天你要转过来用mac的时候不至于完全不会。    快捷键都不一样，怎么办？\n CheatSheet for mac 可以帮到你。 B站视频介绍    macos的反斜杠\\怎么打出来。\n option + ¥    macOS有什么优势。\n 有一个原生的Linux内核，命令行比较好用，不用担心中毒。电池比较好。跟手机一样，可以永远不关机，打开屏幕就能用，记得充一下电就好。    macOS 哪些自带的软件好用 基本都好用，除了Safari，我不用以外。\n mail：邮件可以登录很多邮箱，然后按你喜欢的方法标记不同颜色的旗子，加vip邮箱，等你收到几千封邮件的时候你就知道找邮件有多难了，这样的标签分类方法非常好用，我也把这个方法教给了叶老师，她亲测好用。 Calendar：把日程写在日历上，这样会很容易安排日程，如果不用脑记事情，就能用脑搞科研了。 preview 自带的pdf浏览器，很好用，标记文字，还可以做电子签名。 Finder 文件浏览器，长按空格可以快速浏览。 Notes 随手记一下东西可以用，虽然这个软件也挺强大的了，但是还远远不够。  效率工作 Alumni mail 这是筑波大学校友终身邮箱，自带所有企业版Google用户的服务内容。如：\n 空间无限的 虚拟云端硬盘，这个可以就像本地硬盘一样用，但是不会占多少磁盘空间，重点推荐。 空间无限云相册，手机相册原图无限空间备份。 邮箱 其他  如果还没有注册，在这里 注册。\ngoogle drive 就是上面的 虚拟云端硬盘。这个有什么用，可以把你的所有资料全部放进去。这就不存在你的资料在你的另一台电脑里面，要是找一个文件，就要换电脑找这种问题了。 而且这个用起来就像随时插了一块移动硬盘一样，速度也很快。我写程序都在里面直接写，这样笔记本和台式机切换着用，文件内容也是一样的。非常方便。 而且所有文件都在云端，那么就算身边只有一台手机也能随时查看文件了。还很容易可以共享文件。 这样的好处是，换了电脑也不用换考虑文件怎么转移的问题。电脑坏了也不用担心文件丢失。\n还有一种使用方法，是把工程文件存在里面，直接用Google Colab访问Google drive，运算和储存都在云端，非常方便，我跟进一些项目都是这样写的。\nChrome extensions 要装这个浏览器吗？推荐。因为能装插件。\n推荐安装的插件清单：\n adbolck 屏蔽广告，很好用。 google scholar 谷歌学术，可以方便查论文。 tab wrangler 自动关闭长期不用的选项卡。 read aloud 可以朗读屏幕内容，各种语言都可以，读得很好。 vimium 把浏览器变成vim操作的，学会了会方便一点。  Mendeley 文献管理软件  自动分析题目，作者，摘要。 自动备份论文到云端 换电脑不怕丢，只要账号不丢的话。 利用它的 Chrome插件。网上看到的论文可以装插件一键批量收藏到软件里面，如果有pdf也会自己收集，而且云同步，建立自己的论文库，非常好用。  做笔记怎么做  首先，要想清楚我们每天要高效地工作，主要靠什么。难道靠脑子来记吗。  当项目中有很多事情都不清楚的时候，如何厘清头绪，这个时候不要靠动脑，连事情都搞不懂的时候，就要先动手。把其中的细节一个一个整理出来，了解细节的基础上，分析和思考才有用。  \u0008那么怎么做？  当然是做笔记了，只有做笔记才能解放大脑来思考。       然后，怎么做笔记。  用纸来做笔记，手写不伤眼睛，还可以写写画画。 电子笔记，又可以文字，还可以图片，还可以表格。 问题来了，哪种笔记最可以帮助思考。  表格，不管是电子的还是纸质的，表格笔记都是最好的。  哪种表格最可以帮助思考。  黄金三分法 B站视频教程，方格笔记术          在线笔记怎么记。  打开就写。 加个标题写。找个模板写。 哪个好？  找模板写好。模板是别人智慧的结晶，事半功倍的关键。   在线笔记哪家强。  notion \u0026gt; evernote \u0026gt; others。我是evernote的付费老用户，直到见到notion了以后。notion特别是表格特别方便，就像我上面提到的，把各种细节放在表格一行里面，这样看上去一览无余，非常轻松，这是非常方便的。 notion官方教程 如何提高科研效率？博士小姐姐的科研流水线｜效率软件Notion使用方法     最后，读论文怎么做笔记。  如果你像我一样没有什么高科技的书写设备。打印出来，在上面写批注是最简单的方式。 写哪些批注?  重要参考文献的题目，应该标注，否则就不知道他表达的是什么。 不会的单词意思。 疑问。重中之重。有疑问是最好的，哪里不懂，哪里就是突破点，赶快标注出来。等全文读完之后，就知道接下来要学习的方向在哪里了。 自己给文章分段，写出每段的小标题。这样就对文章结构清晰了。      写程序用什么ide vscode 支持语言多，另外可以直接访问服务器，直接在服务器上写代码，是不是很厉害。\n写论文  Overleaf 在线latex 还可以多人协作 Grammarly 语法检查, 办了会员还可以查重。我有时会去淘宝买一个会员来用。 Manchester Academic Phrasebank 偶尔有用，可以查各种时候的表达句型。  学术圈工具=论文宣传手段 这里重点是，要多在学术社交媒体上曝光，增加出镜机会。最好的方法就是公开代码，并且让大家一下载就能运行试试看，只要能得出跟论文接近的结果就行，代码可以先公开然后慢慢整理。\n  google scholar 谷歌学术  resarchgate 学术圈社交网络  researchmap 日本学术圈社交网络  linkedin 职业圈社交网络。等你得到奖学金，有RA都可以在上面更新，临近毕业的时候会有猎头来介绍工作的。  github 这个是要经营的，我没有管它，所以没啥亮点。  paperwithcode 论文和代码展示的一个网站，这里面还有一些排名什么的，显示目前在一些公开数据集上的最高分数，如果刷到高分，会有很多人看你的论文，和代码的。  ","date":1619172872,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619172872,"objectID":"f5f670a62df4fce91ae393b51fe8aa29","permalink":"https://li-hongmin.github.io/post/toolboxes/","publishdate":"2021-04-23T19:14:32+09:00","relpermalink":"/post/toolboxes/","section":"post","summary":"给研究室新人的一些工具选择上的建议。","tags":[],"title":"Researcher's Toolboxes","type":"post"},{"authors":[],"categories":[],"content":"","date":1602666665,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602666665,"objectID":"e673ded6d076269015efd539757817ae","permalink":"https://li-hongmin.github.io/project/research_paper/","publishdate":"2020-10-14T18:11:05+09:00","relpermalink":"/project/research_paper/","section":"project","summary":"My papers","tags":["RP"],"title":"Research Papers","type":"project"},{"authors":["Hongmin Li","Xiuca Ye","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1602666364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602666364,"objectID":"c80cc156bb8a48a9dfdbfd5d83b812b3","permalink":"https://li-hongmin.github.io/publication/elsc/","publishdate":"2020-10-14T18:06:04+09:00","relpermalink":"/publication/elsc/","section":"publication","summary":"Ensemble clustering has attracted much attention in machine learning and data mining for the high performance in the task of clustering. Spectral clustering is one of the most popular clustering methods and has superior performance compared with the traditional clustering methods. Existing ensemble clustering methods usually directly use the clustering results of the base clustering algorithms for ensemble learning, which cannot make good use of the intrinsic data structures explored by the graph Laplacians in spectral clustering, thus cannot obtain the desired clustering result. In this paper, we propose a new ensemble learning method for spectral clustering-based clustering algorithms. Instead of directly using the clustering results obtained from each base spectral clustering algorithm, the proposed method learns a robust presentation of graph Laplacian by ensemble learning from the spectral embedding of each base spectral clustering algorithm. Finally, the proposed method applies k-means on the spectral embedding obtain from the learned graph Laplacian to get clusters. Experimental results on both synthetic and real-world datasets show that the proposed method outperforms other existing ensemble clustering methods.","tags":[],"title":"[ICMD 2020] Ensemble Learning for Spectral Clustering","type":"publication"},{"authors":["Hongmin Li","Xiucai Ye","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1602665880,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602665880,"objectID":"4b38c8b531aa9fd5e1f8d3bd81eb6fa8","permalink":"https://li-hongmin.github.io/publication/hnsc/","publishdate":"2020-07-19T17:58:00+09:00","relpermalink":"/publication/hnsc/","section":"publication","summary":"An advanced Nyström spectral clustering methods based on Hubness.","tags":["Sampling methods","Matrix decomposition","Clustering methods","Sparse matrices","Approximation error","Mathematical model","Computational complexity"],"title":"[IJCANN 2020] Hubness-based Sampling Method for Nystrom Spectral Clustering ","type":"publication"},{"authors":["Hongmin Li"],"categories":["tools"],"content":"Toolbox list   A matlab project template for clustering research  Toolboxes for A researcher  ","date":1593514681,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593514681,"objectID":"bc0fc37708e668f92f9f0be5fd0801e3","permalink":"https://li-hongmin.github.io/project/toolbox/","publishdate":"2020-06-30T19:58:01+09:00","relpermalink":"/project/toolbox/","section":"project","summary":"Made by myself","tags":["toolbox"],"title":"My Toolbox","type":"project"},{"authors":[],"categories":[],"content":"A matlab project template for clustering research After I finished a research paper on clustering, I found the project management is very important to ensure that the project can smoothly go.\nThat\u0026rsquo;s why I need a project template so that I can repeatly use it to conduct experiments in a short period.\n Here is a mindmap of this project. I create a clear structure to manage source code and the results.\nIn additon, the project can automatically generate latex tables to show the results.\nThe todolist is very usefull during conducting experiments, which lets you make sure which step you are working for.\nThe githus link is https://github.com/Li-Hongmin/Matlab_Project_Template_For_Research_Paper.git\n","date":1593513235,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593513235,"objectID":"9985395ded843645a6badcfb1e25ca55","permalink":"https://li-hongmin.github.io/post/200630_matlab_templete_clustering_project/","publishdate":"2020-06-30T19:33:55+09:00","relpermalink":"/post/200630_matlab_templete_clustering_project/","section":"post","summary":"Experimental framework","tags":["MATLAB","github","clustering"],"title":"A matlab project template for clustering research","type":"post"},{"authors":["Xiucai Ye","Hongmin Li","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1582180773,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582180773,"objectID":"9ac9b6a14953bc7241bf8f8bfdd2d145","permalink":"https://li-hongmin.github.io/publication/oversampling/","publishdate":"2020-02-20T15:39:33+09:00","relpermalink":"/publication/oversampling/","section":"publication","summary":"Imbalanced classification is a challenging problem in machine learning and data mining. Oversampling methods, such as the Synthetic Minority Oversampling Technique (SMOTE), generate synthetic data to achieve data balance for imbalanced classification. However, such kind of oversampling methods generates unnecessary noise when the data are not well separated. On the other hand, there are many applications with inadequate training data and vast testing data, making the imbalanced classification much more challenging. In this paper, we propose a novel oversampling framework to achieve the following two objectives. (1) Improving the classification results of the SMOTE based oversampling methods; (2) Making the SMOTE based oversampling methods applicable when the training data are inadequate. The proposed framework utilizes the Laplacian eigenmaps to find an optimal dimensional space, where the data are well separated and the generation of noise by SMOTE based oversampling methods can be avoided. The construction of graph Laplacian not only explores the useful information from the unlabeled testing data to facilitate imbalanced learning, but also makes the learning process incremental. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed framework.","tags":[],"title":"An Oversampling Framework for Imbalanced Classification Based on Laplacian Eigenmaps","type":"publication"},{"authors":[],"categories":[],"content":"You can find this paper with its doi.\nLet\u0026rsquo;s try to read the abstract together.\nAbstract Definitions of Ensemble Clustering (EC) Ensemble Clustering (EC) aims to integrate multiple Basic Partitions (BPs) of the same dataset into a consensus one. It could be transformed as a graph partition problem on the co-association matrix derived from BPs.\nProblem Existing EC methods usually directly use the co-association matrix, yet without considering various noises (e.g., the disagreement between different BPs or outliers) that may exist in it. These noises can impair the cluster structure of a co-association matrix and thus degrade the final clustering performance.\nWell, you almost can say everything without considering the noises.\nMethod : Robust Spectral Ensemble Clustering (RSEC)  RSEC learns a robust representation for the co-association matrix through low-rank constraint, which reveals the cluster structure of a co-association matrix and captures various noises in it. RSEC finds the consensus partition by conducting spectral clustering.  These two steps are iteratively performed in a unified optimization framework.\nFigure 1: An illustration of the proposed $RSEC$, which simultaneously learns a low-rank representation $Z$ for the co-association matrix $S$ and finds the consensus partition $H$ by conducting spectral clustering on $L_z$. We employ $Z$ to reveal the cluster structure of $S$, and capture the noises inside $S$ with a sparse error matrix $E$. During the learning process, we use $H$ to iteratively enhance the block-diagonal structure of $Z$. The final clustering result could be obtained either from H or $Z$.\nOther details: Most importantly, during our optimization process, we utilize consensus partition to iteratively enhance the block-diagonal structure of the learned representation to further assist the clustering process. Experiments on numerous realworld datasets demonstrate the effectiveness of our method compared with the state-of-the-art. Moreover, several impact factors that may affect the clustering performance of our approach are also explored extensively.\n1. Somethings in Introduction: The contributions of this work are summarized as three-folds:\n  A unified optimization framework is provided to simultaneously learn a robust representation for the co-association matrix and find the final consensus partition.\n  The block-diagonal structure of the learned representation is iteratively enhanced by a consensus partition during the optimization process, thus it better uncovers the cluster structure of the co-association matrix.\n  But, if the order is random, block-diagonal structure may not be found, right?\nExperiments conducted on twelve real-wold datasets demonstrate the effectiveness of our approach over the state-of-the-art EC methods. Moreover, several impact factors that may affect the clustering performance of RSEC are also explored extensively.  2. Somethings in related work 2.1 Ensemble Clustering The first category employs a utility function to measure the similarity between the consensus clustering and multiple BPs, and usually finds the final partition by maximizing an explicit objective function.\n For instance,   For instance, Topchy et al.[27] proposed a Quadratic Mutual Information based objective function for consensus clustering, and used K-means clustering to find the solution. They further extended their work to using the EM algorithm with a finite mixture of multinomial distributions for consensus clustering [28]. Along this line, Wu et al.[32] transferred the ensemble clustering into a K-means clustering problem with KCC utility function and gave the necessary and sufficient conditions for KCC utility functions. In addition, there are some other interesting objective functions for the ensemble clustering, such as the ones based on nonnegative matrix factorization [12], kernel-based methods [30], and simulated annealing [18], respectively.\n The second category summarizes the information of input BPs into a co-association matrix, which counts how many times two instances occur in the same cluster. The co-association matrix ac- tually represents the pairwise similarity of all the data points in the partition space. Thus, a graph partition algorithm can be conducted on it to obtain the final clustering result.\n For instance,   Strehl and Ghosh [26] developed three graph-based algorithms for consensus clustering, while Fred and Jain [8] applied the agglomerative hierarchical clus- tering. Recently, Liu et al.[16] proposed a spectral ensemble clus- tering method, which ran spectral clustering on the co-association matrix and transformed it as a weighted K-means problem to achieve high efficiency. Other methods include Relabeling and Voting [1], Locally Adaptive Cluster based methods [6], genetic algorithm based methods [38], and still many more.\n 2.2 Low-Rank Matrix Analysis LRR [15, 14] assumes the data are drawn from a union of multiple low-dimensional subspaces, and tries to recover these subspaces by seeking the lowest rank representation Z for X as:\n $$ \\min _{\\mathbf{Z}, \\mathbf{E}} \\operatorname{rank}(\\mathbf{Z})+\\lambda\\|\\mathbf{E}\\|_{0} \\text { s.t.} \\mathbf{X}=\\mathbf{X} \\mathbf{Z}+\\mathbf{E} $$  where λ  0 balances the rankness of Z and the sparseness of the error matrix E. Since Eq.(1) is a NP-hard problem, we usually solve its convex relaxation by using nuclear norm to estimate the rank(Z) and l1 or l2, 1 norm to approximate ∥E∥0. Actually, since the minimizer of Eq.(1) also obtains a low-rank matrix XZ for X, LRR could be seen as a generalization for the Robust PCA [14]. It is worthy to note that, by utilizing X to express itself, Z is actu- ally a similarity matrix that reveals the membership between data points. Moreover, it has been proven that Z enjoys a nice block- diagonal property [14, 37], which can uncover the global structure of data and further facilitate the clustering task. Therefore, in this paper, we employ LRR to learn a robust representation for the co- association matrix, rather than directly recover it as a low-rank one.\nEmmy, I guess this is important for this paper. I\u0026rsquo;m not very clear what author said. It seems works.\n3. ROBUST SPECTRAL ENSEMBLE CLUSTERING Let X = {x1, x2, ··· , xn} be a set of n data points independently sampled from K clusters, represented as C = {C1 , · · · , Ck }.\nDenote Π = {π1, π2, ··· , πr} as r input basic partitions (BPs), each of which divides X into Ki crisply partitions and maps X into a label set πi = {πi(x1), πi(x2), · · · , πi(xn)}, where Ki is the cluster number for the i-th BP, 1 ≤ πi(xj) ≤ Ki, and 1 ≤ i ≤ r, 1 ≤ j ≤ n.\nNote that, the cluster number of each BP is usually set to be different for achieving the diversity among input BPs, which has been recognized as an efficient manner to ensure the success for ensemble clustering [1, 32].\nAgree\nThe goal of ensemble clustering is to find the consensus parti- tion that agrees with input BPs most and divides X into its origi- nal K clusters. Commonly, EC methods may summarize r BPs as a co-association matrix, and then conduct a graph partition algo- rithm to obtain the final consensus clustering, denoted as π. The co-association matrix S ∈ Rn×n actually calculates the times of two instances occurring in the same cluster based on Π, which is defined as [8]:\n $$ \\mathbf{S}\\left(x_{p}, x_{q}\\right)=\\sum_{i=1}^{r} \\delta\\left(\\pi_{i}\\left(x_{p}\\right), \\pi_{i}\\left(x_{q}\\right)\\right) $$  where $x_{p}, x_{q} \\in \\mathcal{X}$ and δ(a, b) is 1 if a = b; 0 otherwise. Obvi- ously, S could be normalized by S = S/r. Inspired by [16], we apply the spectral clustering on the co-association matrix S, and have its trace minimization form by following [19] as\n $$ \\min _{\\mathbf{H}} \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{s} \\mathbf{H}\\right) \\text { s.t.} \\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}, $$   $$ \\mathbf{L}_{s}=\\mathbf{I}-\\mathbf{D}_{s}^{-1 / 2} \\mathbf{S} \\mathbf{D}_{s}^{-1 / 2} $$  of S, with degree matrix $D_s$ ∈ Rn×n being a diagonal matrix whose jth diagonal element is the sum of the jth row of S, and H ∈ Rn×K is defined as the scaled partition matrix of π:\n $$ \\mathbf{H}_{j k}=\\left\\{\\begin{array}{ll}{1 / \\sqrt{\\left|C_{k}\\right|}, } \u0026 {\\text { if } x_{j} \\in C_{k} \\text { in } \\pi} \\\\ {0, } \u0026 {\\text { otherwise }}\\end{array}\\right. $$  We can use H to represent the final ensemble clustering result.\nReally? interesting, this is first time I read this.\n3.2 Problem Formulation Given a normalized co-association matrix S, the objective func- tion of our RSEC is formulated as:\n $$ \\begin{array}{l}{\\min _{\\mathbf{H}, \\mathbf{z}, \\mathbf{E}} \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{z} \\mathbf{H}\\right)+\\lambda_{1}\\|\\mathbf{Z}\\|_{*}+\\lambda_{2}\\|\\mathbf{E}\\|_{2, 1}} \\\\ {\\text { s.t.} \\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}, \\mathbf{S}=\\mathbf{S} \\mathbf{Z}+\\mathbf{E}}\\end{array} $$  with $$ \\mathbf{L}_{z}=\\mathbf{I}-\\mathbf{D}_{z}^{-1 / 2}\\left(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2+\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\right) \\mathbf{D}_{z}^{-1 / 2} $$ where \\(\\mathbf{H}\\) denotes the consensus partition, \\(\\mathbf{Z} \\in \\mathbb{R}^{n \\times n}\\) is the learned representation, \\(\\mathbf{E} \\in \\mathbb{R}^{n \\times n}\\) is an error matrix that tries to capture various noises inside \\(\\mathbf{S}, \\) and \\(\\lambda_{1}, \\lambda_{2}0\\) are two penalty parameters to balance the corresponding terms. In Eq.(5), \\(\\mathbf{L}_{z}\\) is designed as a normalized Laplacian matrix of the graph constructed by \\(\\mathbf{Z}\\) and \\(\\mathbf{H}, \\) and the degree matrix \\(\\mathbf{D}_{z}\\) is computed by:   $$ \\mathbf{L}_{z}=\\mathbf{I}-\\mathbf{D}_{z}^{-1 / 2}\\left(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2+\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\right) \\mathbf{D}_{z}^{-1 / 2} $$   where \\(d_{j}, 1 \\leq j \\leq n, \\) is the sum of the \\(j\\) -th row of the matrix \\(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2+\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\). Here, we employ \\(\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) / 2\\) instead of \\(\\mathbf{Z}\\) to achieve a symmetric graph. Moreover, since \\(\\mathbf{H}\\) is a high-quality clustering result, \\(\\mathbf{H} \\mathbf{H}^{\\mathrm{T}}\\) enjoys a clear cluster structure. Thus, we use it to further enhance the block-diagonal structure of \\(\\mathbf{Z}\\).  3.3 Optimization : The Augmented Lagrange Multiplier (ALM) The Augmented Lagrange Multiplier (ALM) method [13] comes to mind as an efficient and effective solver to our problem. To apply ALM, we first introduce an auxiliary variable J to make Eq.(5) separable, and equivalently convert it as:\n $$ \\begin{array}{ll}{} \u0026 {\\min _{\\mathbf{H}, \\mathbf{Z}, \\mathbf{E}} \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{z} \\mathbf{H}\\right)+\\lambda_{1}\\|\\mathbf{J}\\|_{*}+\\lambda_{2}\\|\\mathbf{E}\\|_{2, 1}} \\\\ {} \u0026 {\\text { s.t.} \\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}, \\mathbf{S}=\\mathbf{S} \\mathbf{Z}+\\mathbf{E}, \\mathbf{Z}=\\mathbf{J}}\\end{array} $$   Following \\([5], \\) the constraint \\(\\mathbf{H}^{\\mathrm{T}} \\mathbf{H}=\\mathbf{I}\\) is relaxed to avoid hard partition during the optimization process. Then, the augmented La- grangian function of Eq.( 8) is:   \\(\\begin{aligned} \\mathcal{L}=\u0026 \\operatorname{tr}\\left(\\mathbf{H}^{\\mathrm{T}} \\mathbf{L}_{z} \\mathbf{H}\\right)+\\lambda_{1}\\|\\mathbf{J}\\|_{*}+\\lambda_{2}\\|\\mathbf{E}\\|_{2, 1} \\\\ \u0026+\\left\\langle\\mathbf{Y}_{1}, \\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}\\right\\rangle+\\left\\langle\\mathbf{Y}_{2}, \\mathbf{Z}-\\mathbf{J}\\right\\rangle \\\\ \u0026+\\frac{\\mu}{2}\\left(\\|\\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}\\|_{\\mathrm{F}}^{2}+\\|\\mathbf{Z}-\\mathbf{J}\\|_{\\mathrm{F}}^{2}\\right) \\end{aligned}\\)  where Y1 and Y1 are two Lagrangian multipliers, and μ \u0026gt; 0 is the penalty parameter.\nThe ALM solver solves Eq.(9) with an iterative update manner, which addresses J, Z, E, and H in sequence and optimizes one variable at a time by fixing the others. More details are given in the following.\nUpdata J.  We first minimize (\\mathcal{L}) with respect to (\\mathrm{J}, ) and obtain (\\mathrm{J}^{(t+1)}) by: $$ \\begin{aligned} \\quad \u0026amp; \\text { argmin } \\lambda_{1}|\\mathbf{J}|_{*}+\\left\\langle\\mathbf{Y}_{2}^{(t)}, \\mathbf{Z}^{(t)}-\\mathbf{J}\\right\\rangle+\\frac{\\mu^{(t)}}{2}\\left|\\mathbf{Z}^{(t)}-\\mathbf{J}\\right|_{\\mathrm{F}}^{2} \\\n= \\underset{\\mathbf{J}}{\\operatorname{argmin}} \\frac{\\lambda_{1}}{\\mu^{(t)}}|\\mathbf{J}|_{*}+\\frac{1}{2}\\left|\\mathbf{J}-\\left(\\mathbf{Z}^{(t)}+\\frac{1}{\\mu^{(t)}} \\mathbf{Y}_{2}^{(t)}\\right)\\right|_{\\mathrm{F}}^{2} \\end{aligned} $$\n Eq.(10) could be solved by the Singular Value Thresholding (SVT) operator [3], which has a closed-form solution as:\n $$ \\begin{array}{c}{\\mathbf{J}^{(t+1)}=\\Theta_{\\frac{\\lambda_{1}}{\\mu^{(t)}}}\\left(\\mathbf{Z}^{(t)}+\\frac{1}{\\mu^{(t)}} \\mathbf{Y}_{2}^{(t)}\\right)} \\\\ {\\text { where } \\Theta(\\cdot) \\text { is the SVT operator.}}\\end{array} $$  Update Z  Update \\(\\mathbf{Z} .\\) By substituting Eq.(6) into \\(\\mathcal{L}\\) and dropping unrelated terms, the subproblem for updating \\(\\mathbf{Z}\\) is equivalent to the following: \\\\ argmin \\(-\\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{H}^{(t) T} \\mathbf{D}_{z}^{-1 / 2}\\left(\\mathbf{Z}+\\mathbf{Z}^{\\mathrm{T}}\\right) \\mathbf{D}_{z}^{-1 / 2} \\mathbf{H}^{(t)}\\right)\\) \\(\\quad+\\left(\\mathbf{Y}_{1}^{(t)}, \\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}^{(t)}\\right\\rangle+\\left\\langle\\mathbf{Y}_{2}^{(t)}, \\mathbf{Z}-\\mathbf{J}^{(t+1)}\\right\\rangle\\) \\(\\quad+\\frac{\\mu^{(t)}}{2}\\left(\\left\\|\\mathbf{S}-\\mathbf{S} \\mathbf{Z}-\\mathbf{E}^{(t)}\\right\\|_{\\mathbf{F}}^{2}+\\left\\|\\mathbf{Z}-\\mathbf{J}^{(t+1)}\\right\\|_{\\mathbf{F}}^{2}\\right)\\)   Note that, the derivative of \\(\\mathbf{D}_{z}\\) with respect to \\(\\mathbf{Z}\\) is relatively complex, which actually complicates the solution of obtaining \\(\\mathbf{Z}^{(t+1)} .\\)   By fixing \\(D_{z}, \\) Eq.( 12) becomes a quadratic problem of \\(\\mathbf{Z} .\\) Thus, taking the derivative of \\(\\mathcal{L}\\) with respect to \\(\\mathbf{Z}\\) gives \\(\\mathbf{Z}^{(t+1)}\\) as: \\(\\mathbf{Z}^{(t+1)}=\\left(\\mathbf{S S}^{\\mathrm{T}}+\\mathbf{I}\\right)^{-1}\\left(\\mathbf{S}^{\\mathrm{T}} \\mathbf{S}+\\mathbf{J}^{(t+1)}-\\mathbf{S}^{\\mathrm{T}} \\mathbf{E}^{(t)}+\\right.\\) \\(\\left.\\quad \\frac{1}{\\mu^{(t)}}\\left(\\mathbf{S}^{\\mathrm{T}} \\mathbf{Y}_{1}^{(t)}-\\mathbf{Y}_{2}^{(t)}+\\mathbf{D}_{z}^{-1 / 2} \\mathbf{H}^{(t)} \\mathbf{H}^{(t) \\mathrm{T}} \\mathbf{D}_{z}^{-1 / 2}\\right)\\right)\\)  Update E. Update Multipliers ","date":1577436212,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577436212,"objectID":"13b7e9b08aeaea23632a3b077a51ddab","permalink":"https://li-hongmin.github.io/post/lr2_rsec/","publishdate":"2019-12-27T17:43:32+09:00","relpermalink":"/post/lr2_rsec/","section":"post","summary":"paper note","tags":["Ensemble Clustering; Spectral Clustering; Co-association Matrix; Low-rank Representation"],"title":"Literature Review #2: Robust Spectral Ensemble Clustering","type":"post"},{"authors":[],"categories":[],"content":"This excellent paper was punished by Ana L. N. Fred and Anil K. Jain in 2005 IEEE Transactions on Pattern Analysis and Machine Intelligence. It proposed An clustering ensemble is based on a voting strategy for various partitions. Outline of evidence accumulation clustering algorithm is below: 1. Introduction Here are literature review. All methods are old enough. I guess we can skim it.\nA list of clustering application  A number of application areas use clustering techniques for organizing or discovering structure in data, such as data mining [1], [2], information retrieval [3], [4], [5], image segmentation [6], and machine learning.\n A list of clustering methods  Examples of model-based techniques include:\n parametric density approaches, such as mixture decomposition techniques [23], [24], [25], [26]; prototype-based methods, such as central clustering [14], square-error clustering [27], K-means [28], [8], or K-medoids clustering [9]; and shape fitting approaches [15], [6], [16].  Most of the above techniques utilize an optimization procedure tuned to a particular cluster shape, or emphasize cluster compactness.\n Fisher et al. [31] proposed an optimization-based clustering algorithm, based on a pairwise clustering cost function, emphasizing cluster connectedness.\n  Nonparametric density-based clustering methods attempt to identify high density clusters separated by low density regions [5], [32], [33].\n  Graph-theoretical approaches [34] have mostly been explored in hierarchical methods that can be represented graphically as a tree or dendrogram [7], [8]. Both agglomerative [28], [35] and divisive approaches [36] (such as those based on the minimum spanning tree—MST [28]) have been proposed; different algorithms are obtained depending on the definition of similarity measures between patterns and between clusters [37]. The single-link (SL) and the complete-link (CL) hierarchical methods [7], [8] are the best known techniques in this class, emphasizing, respectively, connectedness and compactness of patterns in a cluster. Prototype-based hierarchical methods, which define similarity between clusters based on cluster representatives, such as the centroid, emphasize compactness. Variations of the prototype-based hierarchical clustering include the use of multiple prototypes per cluster, as in the CURE algorithm [38]. Other hierarchical agglomerative clustering algorithms follow a split and merge technique, the data being initially split into a large number of small clusters, merging being based on intercluster similarity; a final partition is selected among the clustering hierarchy by thresholding techniques are based or measures of cluster validity [39], [5], [40], [41], [42], [43].\n  Treating the clustering problem as a graph partitioning problem, a recent approach, known as spectral clustering, applies spectral graph theory for clustering [44], [45], [46].\n   The characteristic of K-means algorithm   minimizes the squared-error criteria, is one of the simplest clustering algorithm.\n  It is computationally efficient and does not require the user to specify many parameters.\n  Its major limitation, however, is the inability to identify clusters with arbitrary shapes, ultimately imposing hyperspherical shaped clusters on the data.\n  Extensions of the basic K-means algorithm include: use of Mahalanobis distance to identify hyperellipsoidal clusters [28], introducing fuzzy set theory to obtain nonexclusive partitions [20], and adaptations to straight line fitting [47].\n  Problem description While hundreds of clustering algorithms exist, it is difficult to find a single clustering algorithm that can handle all types of cluster shapes and sizes or even decide which algorithm would be the best one for a particular data set [48], [49].\nFigure 1 Results of clusterings using different algorithms (K-means, single-link—SL, and complete-link—CL) with different parameters. Each cluster identified is shown in a different color/pattern.(a) Input data.(b) K-means clustering, k=8.(c) Clustering with the SL method, threshold at 0.55, resulting in 27 clusters.(d) Clustering with the SL method, forcing eight clusters.(e) Clustering with the CL method, threshold at 2.6, resulting in 22 clusters.(f) Clustering with the CL method, forcing eight clusters.\n Related Work and why use k-means as basic partition method   Inspired by the work in sensor fusion and classifier combination [50], [51], [52], a clustering combination approach has been proposed [53], [54], [55]. Fred and Jain introduce the concept of evidence accumulation clustering that maps the individual data partitions in a clustering ensemble into a new similarity measure between patterns, summarizing interpattern structure perceived from these clusterings. The final data partition is obtained by applying the single-link method to this new similarity matrix. The results of this method show that, the combination of “weak” clustering algorithms such as the K-means, which impose a simple structure on the data, can lead to the identification of true underlying clusters with arbitrary shapes, sizes and densities. Strehl and Ghosh [56] explore the concept of consensus between data partitions and propose three different combination mechanisms. The first step of the consensus functions is to transform the data partitions into a hypergraph representation. The hypergraph-partitioning algorithm (HGPA) obtains the combined partition by partitioning the hypergraph into k unconnected components of approximately the same size, by cutting a minimum number of hyperedges. The metaclustering algorithm (MCLA) applies a graph-based clustering to hyperedges in the hypergraph representation. Finally, CSPA uses a pairwise similarity, as defined by Fred and Jain [55], and the final data partition is obtained by applying the METIS algorithm of Karypis and Kumar to the induced similarity measure between patterns.\n 2. Problem Formulation Consider $N$ partitions of the data $X$ and let $(\\mathrm{F})$ represent the set of $N$ partitions, which we define as a clustering ensemble:\n$$\\mathrm{P}=\\left\\{P^{1}, P^{2}, \\ldots, P^{N}\\right\\}$$  $$ \\begin{aligned} P^{1} \u0026=\\left\\{C_{1}^{1}, C_{2}^{1}, \\ldots, C_{k_{1}}^{1}\\right\\} \\\\ \u0026 \\vdots \\\\ P^{N} \u0026=\\left\\{C_{1}^{N}, C_{2}^{N}, \\ldots, C_{k_{N}}^{N}\\right\\} \\end{aligned} $$  where $C_{j}^{i}$ is the $j$ th cluster in data partition $P^{i},$ which has $k_{i}$ clusters and $n_{j}^{i}$ is the cardinality of $C_{j}^{i},$ with $\\sum_{j=1}^{k_{i}} n_{j}^{i}=n, \\quad i=1, \\ldots, N$\nThe problem is to find an \u0026ldquo;optimal\u0026rdquo; data partition, $P^{},$ using the information available in $N$ different data partitions in $\\mathrm{FP}=\\left{P^{1}, P^{2}, \\ldots, P^{N}\\right} .$ We define $k^{}$ as the number of clusters in $P^{} .$ Ideally, $P^{}$ should satisfy the following properties:\n Consistency with the clustering ensemble $\\mathrm{F}$; Robustness to small variations in $\\mathrm{F}$; Goodness of fit with ground truth information (true cluster labels of patterns), if available.  3. Evidence accumulation clustering 3.1 Producing Clustering Ensembles Clustering ensembles can be generated by following two approaches:\n1) choice of data representation In the first approach, different partitions of the objects under analysis may be produced by:\na) employing different preprocessing and/or feature extraction mechanisms, which ultimately lead to different pattern representations (vectors, strings, graphs, etc.) or different feature spaces\nb) exploring subspaces of the same data representation, such as using subsets of features\nc) perturbing the data, such as in bootstrapping techniques (like bagging), or sampling approaches, as, for instance, using a set of prototype samples to represent huge data sets.\n 2) choice of clustering algorithms or algorithmic parameters.   In the second approach, we can generate clustering ensembles by:\ni) applying different clustering algorithms,\nii) using the same clustering algorithm with different parameters or initializations,\niii) exploring different dissimilarity measures for evaluating interpattern relationships, within a given clustering algorithm.\n 3.2 Combining Evidence: The Co-Association Matrix Taking the co-occurrences of pairs of patterns in the same cluster as votes for their association, the $N$ data partitions of $n$ patterns are mapped into a $n \\times n$ co-association matrix: $$ \\mathcal{C}(i, j)=\\frac{n_{i j}}{N} $$\nwhere $n_{i j}$ is the number of times the pattern pair $(i, j)$ is assigned to the same cluster among the $N$ partitions.\nAuthors give an illustration of proposed methods in figure below:\nFig. 2. Individual clusterings and combination results on concentric clusters using the K-means algorithm. (a) Data set with concentric clusters. (b) First run of K-means, k=25. (c) Second run of K-means, k=11. (d) Plot of the interpattern similarity matrix for the data in (a). (e) Co-association matrix for the clustering in (b). (f) Co-association matrix for the clustering in (c). (g) Co-association matrix based on the combination of 30 clusterings. (h) Two-dimensional multidimensional scaling of the co-association matrix in (g). (i) Evidence accumulation data partition.\n3.3 Recovering Natural Clusters The core of the evidence accumulation clustering technique is the mapping of partitions into the co-association matrix, $C$. As figure shows, the clustering is obtained using the a MST-based clustering with co-association matrix.\nFig. 3. Dendrogram produced by the SL method using the similarity matrix in Fig. 2g. Distances (1−similarity) are represented along the graph ordinate. From the dendrogram, the following cluster lifetimes are identified: 2-clusters: l2=0.18, 3-clusters: l3=0.36, 4-clusters: l4=0.14, 5-clusters: 0.02. The 3-cluster partition (shown in Fig. 2i), corresponding to the longest lifetime, is chosen (threshold on the dendrogram is between 0.4 and 0.76).\n","date":1577171080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577171080,"objectID":"b5ce6df147e00c9e2a43d2a2dcf397eb","permalink":"https://li-hongmin.github.io/post/notepaper_combining_multiple_clustering_using_evidence_accumulation/","publishdate":"2019-12-24T16:04:40+09:00","relpermalink":"/post/notepaper_combining_multiple_clustering_using_evidence_accumulation/","section":"post","summary":"paper note","tags":["clustering ensemble","consensus clustering"],"title":"Literature Review #1: Combining multiple clustering using evidence accumulation ","type":"post"},{"authors":[],"categories":[],"content":"学习笔记集合：  介绍了简单的Hugo应用。  跳转到 Day1\n介绍了github pages和部署。  跳转到 Day2\n","date":1577031190,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577031190,"objectID":"30ea716f1df0833734efacb1bc50f758","permalink":"https://li-hongmin.github.io/project/hugo/hugo/","publishdate":"2019-12-23T01:13:10+09:00","relpermalink":"/project/hugo/hugo/","section":"project","summary":"A notebook for using hugo to create my website","tags":["hugo"],"title":"【学习笔记】Hugo 建站","type":"project"},{"authors":[],"categories":[],"content":"上次我们已经做个一个网站在本地了，这次要把它部署到Github pages上。\n本文主要参考 hugo官方文档,需要github和本地两别操作。\n部署前的准备 首先要确认三点：\n 确认本地有无 git:  git --version # git version 2.21.0 (Apple Git-122.2)   确认 GitHub账号有无，可以免费申请一个。\n  已经有一个可以发布的Hugo网站\n  两种GitHub Pages  User/Organization Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io/) Project Pages (https://\u0026lt;USERNAME|ORGANIZATION\u0026gt;.github.io//)  部署设置   在github创建新的pages repository，如果是第一种就把\u0026lt;USERNAME|ORGANIZATION\u0026gt;换成你的用户名或者组织名，则输入如下图：   我因为已经有同名的repository了，所以不可以创建。     再用同样的方法在github创建新的repository用来放hugo的文件，以下用\u0026lt;YOUR-PROJECT\u0026gt;代替这个repository的名字\n  本地git clone，把\u0026lt;YOUR-PROJECT-URL\u0026gt;换成网页url地址，在合适的目录下执行命令: git clone \u0026lt;YOUR-PROJECT-URL\u0026gt; \u0026amp;\u0026amp; cd \u0026lt;YOUR-PROJECT\u0026gt;\n  复制粘贴之前hugo的文件到clone的目录下面。用hugo server来检查下正不正常，浏览器打开 http://localhost:1313。\n  测试过后，用rm -rf public/来移除所有public下的文件。\n  创建一个submodule：git submodule add -b master git@github.com:\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public\n  这个submodule就会同步到\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io，而整个本地目录会同步到\u0026lt;YOUR-PROJECT\u0026gt;这里。\n部署网站 生成网站并部署 hugo cd public git add . git commit -m \u0026quot;Build website\u0026quot; git push origin master cd ..  这样应就可以了，浏览器打开\u0026lt;USERNAME\u0026gt;.github.io就可以看到了。\n自动部署脚本 可以创建一个deploy.sh文件，好像 这样.\n然后记得chmod +x deploy.sh赋予权限。\n只要运行 ./deploy.sh \u0026quot;Your optional commit message\u0026quot;就可以提交更改了。\n总结 流水线作业，总是不够详细。\n另外本网站的制作是根据academic模版，依照其 官网而部署，也很简单。\n","date":1577026400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577026400,"objectID":"162c2b8665175b7eb48685befbbbd570","permalink":"https://li-hongmin.github.io/post/191222_hugo/","publishdate":"2019-12-22T23:53:20+09:00","relpermalink":"/post/191222_hugo/","section":"post","summary":"上次我们已经做个一个网站在本地了，这次要把它部署到Github pages上。 本文主要参考 hugo官方文档,需要github和本地两别操作。 部","tags":["hugo","github pages","personal website"],"title":"【学习笔记】用Hugo建站来写个人博客 Day2","type":"post"},{"authors":[],"categories":[],"content":"建站一直是一个很复杂的工程，不过总有捷径。本文介绍一种快速建立静态网站的工具hugo。hugo是基于go语言新兴静态网站生成工具，非常快速轻便。我们只需要用markdown来写博客直接生成网页，可以让我们专注于内容。另外有超多主题可供选择，换个装潢模版也很容易。\n缘起 我目前在读Ph.D平时读论文时会做一些笔记，一直想把这些笔记整理分享出来。 我于是开始尝试使用个人博客来记录学习过程整理思路，之后还能做一些页面展示研究成果。\n通过一番考察我选择先用 Hugo建立一个网站。 详细Hugo是什么请读者自行了解，我理解就是go语言写的快捷建站工具，并且只要用markdown语言就可以写页面了。 同时在过程中把自己的心得分享出来供后来者参考。\n安装和一键建站 这里我主要参考官网的 Quick Start。\n 安装 Hugo 我的环境时macOS，使用Homebrew安装：  brew install hugo hugo version  最后得到这样的信息：Hugo Static Site Generator v0.61.0/extended darwin/amd64 BuildDate: unknown。\n另外说Homebrew是一个非常好的软件管理软件，一行代码就安装好了，如果还没有试过的人一定要尝试一下。\n如果是Windows或linux用户的话参考 这里进行安装。\n一键建站  在terminal或命令行里找个合适的文件路径，执行以下命令。\nhugo new site yourFolderName  这时Hugo会生成一个网站模版。命令中_yourFolderName_就是文件夹名，可以随便起。\n添加主题  主题是Hugo的一大优势，海量主题任你选。这里是 主题库。\n我们这里就用官方教程里的Ananke。\ncd quickstart # Download the theme git init git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke # Note for non-git users: # - If you do not have git installed, you can download the archive of the latest # version of this theme from: # https://github.com/budparr/gohugo-theme-ananke/archive/master.zip # - Extract that .zip file to get a \u0026quot;gohugo-theme-ananke-master\u0026quot; directory. # - Rename that directory to \u0026quot;ananke\u0026quot;, and move it into the \u0026quot;themes/\u0026quot; directory. # End of note for non-git users. # Edit your config.toml configuration file # and add the Ananke theme. echo 'theme = \u0026quot;ananke\u0026quot;' \u0026gt;\u0026gt; config.toml  添加页面  生成第一个页面。\nhugo new posts/my-first-post.md  第一个页面看起来像这样：\n--- title: \u0026quot;My First Post\u0026quot; date: 2019-03-26T08:47:11+01:00 draft: true ---  可以用你喜欢的IDE来写markdown文件，直接就可以生成网站了。\n运行Hugo服务器  终于到来一键生成网站的时候了，虽然这里的一键是一行命令。\nhugo server -D  最后提示\nWeb Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop  浏览器打开 http://localhost:1313/看看是不是成功了。\n看上去就像是 这样子。\n总结 简单易学，一行命令完事。\n建站大坑，容我慢慢来填。\n","date":1577025505,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577025505,"objectID":"0b0fe5b12b6b85ce0eecc58d978c183e","permalink":"https://li-hongmin.github.io/post/191217_hugo/","publishdate":"2019-12-22T23:38:25+09:00","relpermalink":"/post/191217_hugo/","section":"post","summary":"建站一直是一个很复杂的工程，不过总有捷径。本文介绍一种快速建立静态网站的工具hugo。hugo是基于go语言新兴静态网站生成工具，非常快速轻","tags":["hugo","blog","建站，博客"],"title":"【学习笔记】用Hugo建站来写个人博客 Day1","type":"post"},{"authors":["Xiucai Ye","Hongmin Li","Tetsuya Sakurai","Pei-Wei Shueng"],"categories":[],"content":"","date":1577007783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577007783,"objectID":"5200b2210c2d271d03c2eac782f8f257","permalink":"https://li-hongmin.github.io/publication/ensemble_feature_learning_to_identify_risk_factors_for_predicting_secondary_cancer/","publishdate":"2019-12-22T18:43:03+09:00","relpermalink":"/publication/ensemble_feature_learning_to_identify_risk_factors_for_predicting_secondary_cancer/","section":"publication","summary":"An effective ensemble feature learning method to identify the risk factors for predicting secondary cancer by considering class imbalance and patient heterogeneity.","tags":["secondary cancer","risk factors","class imbalance","patient heterogeneity","spectral clustering","ensemble learning"],"title":"Ensemble Feature Learning to Identify Risk Factors for Predicting Secondary Cancer","type":"publication"},{"authors":["Xiucai Ye","Hongmin Li","Akira Imakura","Tetsuya Sakurai"],"categories":[],"content":"","date":1576898274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576898274,"objectID":"612195691a61badc10a9bf828817b090","permalink":"https://li-hongmin.github.io/publication/distributed_collaborative_feature_selection_based_on_intermediate_representation/","publishdate":"2019-08-10T12:17:54+09:00","relpermalink":"/publication/distributed_collaborative_feature_selection_based_on_intermediate_representation/","section":"publication","summary":"A novel distributed method which allows collaborative feature selection for multiple parties without revealing their original data.","tags":["Unsupervised Learning","Feature Selection","Learning Sparse Models","Dimensionality Reduction and Manifold Learning"],"title":"[IJCAI 2019] Distributed collaborative feature selection based on intermediate representation","type":"publication"},{"authors":["Xiucai Ye","Hongmin Li","Tetsuya Sakurai","Zhi Liu"],"categories":["Spectral Clustering"],"content":"","date":1576849824,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576849824,"objectID":"4809cbd1424ea9165d0ab082220e5751","permalink":"https://li-hongmin.github.io/publication/lsch/","publishdate":"2018-10-08T00:00:00Z","relpermalink":"/publication/lsch/","section":"publication","summary":"An accelerated spectral clustering method based on sparse presentation where each data point is presented as sparse linear combinations of a part of representative data points.","tags":["Spectral Clustering","Hubness","Large Scale","Sparse Representation"],"title":"Large Scale Spectral Clustering Using Sparse Representation Based on Hubness","type":"publication"}]