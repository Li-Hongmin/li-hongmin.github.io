<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Li Hongmin （李鸿敏）</title>
    <link>https://li-hongmin.github.io/</link>
      <atom:link href="https://li-hongmin.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Li Hongmin （李鸿敏）</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><lastBuildDate>Wed, 14 Oct 2020 18:11:05 +0900</lastBuildDate>
    <image>
      <url>https://li-hongmin.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Li Hongmin （李鸿敏）</title>
      <link>https://li-hongmin.github.io/</link>
    </image>
    
    <item>
      <title>Research Papers</title>
      <link>https://li-hongmin.github.io/project/research_paper/</link>
      <pubDate>Wed, 14 Oct 2020 18:11:05 +0900</pubDate>
      <guid>https://li-hongmin.github.io/project/research_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[ICMD 2020] Ensemble Learning for Spectral Clustering</title>
      <link>https://li-hongmin.github.io/publication/elsc/</link>
      <pubDate>Wed, 14 Oct 2020 18:06:04 +0900</pubDate>
      <guid>https://li-hongmin.github.io/publication/elsc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[IJCANN 2020] Hubness-based Sampling Method for Nystrom Spectral Clustering </title>
      <link>https://li-hongmin.github.io/publication/hnsc/</link>
      <pubDate>Wed, 14 Oct 2020 17:58:00 +0900</pubDate>
      <guid>https://li-hongmin.github.io/publication/hnsc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>My Toolbox</title>
      <link>https://li-hongmin.github.io/project/toolbox/</link>
      <pubDate>Tue, 30 Jun 2020 19:58:01 +0900</pubDate>
      <guid>https://li-hongmin.github.io/project/toolbox/</guid>
      <description>&lt;h1 id=&#34;toolbox-list&#34;&gt;Toolbox list&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://li-hongmin.github.io/post/200630_matlab_templete_clustering_project/&#34;&gt;A matlab project template for clustering research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A matlab project template for clustering research</title>
      <link>https://li-hongmin.github.io/post/200630_matlab_templete_clustering_project/</link>
      <pubDate>Tue, 30 Jun 2020 19:33:55 +0900</pubDate>
      <guid>https://li-hongmin.github.io/post/200630_matlab_templete_clustering_project/</guid>
      <description>&lt;h1 id=&#34;a-matlab-project-template-for-clustering-research&#34;&gt;A matlab project template for clustering research&lt;/h1&gt;
&lt;p&gt;After I finished a research paper on clustering, I found the project management is very important to ensure that the project can smoothly go.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why I need a project template so that I can repeatly use it to conduct experiments in a short period.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Here is a mindmap of this project.
I create a clear structure to manage source code and the results.&lt;/p&gt;
&lt;p&gt;In additon, the project can automatically generate latex tables to show the results.&lt;/p&gt;
&lt;p&gt;The todolist is very usefull during conducting experiments, which lets you make sure which step you are working for.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Li-Hongmin/Matlab_Project_Template_For_Research_Paper/raw/master/docs/experiment_framwork.png&#34; alt=&#34;Mindmap&#34;&gt;&lt;/p&gt;
&lt;p&gt;The githus link is
&lt;a href=&#34;https://github.com/Li-Hongmin/Matlab_Project_Template_For_Research_Paper.git&#34;&gt;https://github.com/Li-Hongmin/Matlab_Project_Template_For_Research_Paper.git&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Literature Review #2: Robust Spectral Ensemble Clustering</title>
      <link>https://li-hongmin.github.io/post/lr2_rsec/</link>
      <pubDate>Fri, 27 Dec 2019 17:43:32 +0900</pubDate>
      <guid>https://li-hongmin.github.io/post/lr2_rsec/</guid>
      <description>&lt;p&gt;You can find this paper with its 
&lt;a href=&#34;https://dl.acm.org/citation.cfm?doid=2783258.2783287&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to read the abstract together.&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;h3 id=&#34;definitions-of-ensemble-clustering-ec&#34;&gt;Definitions of Ensemble Clustering (EC)&lt;/h3&gt;
&lt;p&gt;Ensemble Clustering (EC) aims to integrate multiple Basic Partitions (BPs) of the same dataset into a consensus one. It could be transformed as a graph partition problem on the &lt;strong&gt;co-association matrix&lt;/strong&gt; derived from BPs.&lt;/p&gt;
&lt;h3 id=&#34;problem&#34;&gt;Problem&lt;/h3&gt;
&lt;p&gt;Existing EC methods usually directly use the co-association matrix, yet &lt;strong&gt;without considering various noises&lt;/strong&gt; (e.g., the disagreement between different BPs or outliers) that may exist in it. These noises can impair the cluster structure of a co-association matrix and thus degrade the final clustering performance.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Well, you almost can say everything without considering the noises.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;method--robust-spectral-ensemble-clustering-rsec&#34;&gt;Method : Robust Spectral Ensemble Clustering (RSEC)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;RSEC learns a robust representation for the co-association matrix through low-rank constraint, which reveals the cluster structure of a co-association matrix and captures various noises in it.&lt;/li&gt;
&lt;li&gt;RSEC finds the consensus partition by conducting spectral clustering.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two steps are iteratively performed in a unified optimization framework.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;figure&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1: An illustration of the proposed $RSEC$, which simultaneously learns a low-rank representation $Z$ for the co-association matrix $S$ and finds the consensus partition $H$ by conducting spectral clustering on $L_z$. We employ $Z$ to reveal the cluster structure of $S$, and capture the noises inside $S$ with a sparse error matrix $E$. During the learning process, we use $H$ to iteratively enhance the block-diagonal structure of $Z$. The final clustering result could be obtained either from H or $Z$.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;other-details&#34;&gt;Other details:&lt;/h3&gt;
&lt;p&gt;Most importantly, during our optimization process, we utilize consensus partition to iteratively enhance the block-diagonal structure of the learned representation to further assist the clustering process. Experiments on numerous realworld datasets demonstrate the effectiveness of our method compared with the state-of-the-art. Moreover, several impact factors that may affect the clustering performance of our approach are also explored extensively.&lt;/p&gt;
&lt;h2 id=&#34;1-somethings-in-introduction&#34;&gt;1. Somethings in Introduction:&lt;/h2&gt;
&lt;p&gt;The contributions of this work are summarized as three-folds:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A unified optimization framework is provided to simultaneously learn a robust representation for the co-association matrix and find the final consensus partition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The block-diagonal structure of the learned representation is iteratively enhanced by a consensus partition during the optimization process, thus it better uncovers the cluster structure of the co-association matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;But, if the order is random, block-diagonal structure may not be found, right?&lt;/em&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Experiments conducted on twelve real-wold datasets demonstrate the effectiveness of our approach over the state-of-the-art EC methods. Moreover, several impact factors that may affect the clustering performance of RSEC are also explored extensively.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-somethings-in-related-work&#34;&gt;2. Somethings in related work&lt;/h2&gt;
&lt;h3 id=&#34;21-ensemble-clustering&#34;&gt;2.1 Ensemble Clustering&lt;/h3&gt;
&lt;p&gt;The first category employs a utility function to measure the similarity between the consensus clustering and multiple BPs, and usually finds the final partition by maximizing an explicit objective function.&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; For instance, &lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;For instance, Topchy et al.[27] proposed a Quadratic Mutual Information based objective function for consensus clustering, and used K-means clustering to find the solution. They further extended their work to using the EM algorithm with a finite mixture of multinomial distributions for consensus clustering [28]. Along this line, Wu et al.[32] transferred the ensemble clustering into a K-means clustering problem with KCC utility function and gave the necessary and sufficient conditions for KCC utility functions. In addition, there are some other interesting objective functions for the ensemble clustering, such as the ones based on nonnegative matrix factorization [12], kernel-based methods [30], and simulated annealing [18], respectively.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;The second category summarizes the information of input BPs into a co-association matrix, which counts how many times two instances occur in the same cluster. The co-association matrix ac- tually represents the pairwise similarity of all the data points in the partition space. Thus, a graph partition algorithm can be conducted on it to obtain the final clustering result.&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; For instance, &lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;Strehl and Ghosh [26] developed three graph-based algorithms for consensus clustering, while Fred and Jain [8] applied the agglomerative hierarchical clus- tering. Recently, Liu et al.[16] proposed a spectral ensemble clus- tering method, which ran spectral clustering on the co-association matrix and transformed it as a weighted K-means problem to achieve high efficiency. Other methods include Relabeling and Voting [1], Locally Adaptive Cluster based methods [6], genetic algorithm based methods [38], and still many more.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;22-low-rank-matrix-analysis&#34;&gt;2.2 Low-Rank Matrix Analysis&lt;/h3&gt;
&lt;p&gt;LRR [15, 14] assumes the data are drawn from a union of multiple low-dimensional subspaces, and tries to recover these subspaces by seeking the lowest rank representation Z for X as:&lt;/p&gt;
&lt;div&gt;
$$
\min _{\mathbf{Z}, \mathbf{E}} \operatorname{rank}(\mathbf{Z})+\lambda\|\mathbf{E}\|_{0} \text { s.t.} \mathbf{X}=\mathbf{X} \mathbf{Z}+\mathbf{E}
$$
&lt;/div&gt;
where λ &gt; 0 balances the rankness of Z and the sparseness of the error matrix E.
&lt;p&gt;Since Eq.(1) is a NP-hard problem, we usually solve its convex relaxation by using nuclear norm to estimate the rank(Z) and l1 or l2, 1 norm to approximate ∥E∥0. Actually, since the minimizer of Eq.(1) also obtains a low-rank matrix XZ for X, LRR could be seen as a generalization for the Robust PCA [14]. It is worthy to note that, by utilizing X to express itself, Z is actu- ally a similarity matrix that reveals the membership between data points. Moreover, it has been proven that Z enjoys a nice block- diagonal property [14, 37], which can uncover the global structure of data and further facilitate the clustering task. Therefore, in this paper, we employ LRR to learn a robust representation for the co- association matrix, rather than directly recover it as a low-rank one.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Emmy, I guess this is important for this paper. I&amp;rsquo;m not very clear what author said. It seems works.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-robust-spectral-ensemble-clustering&#34;&gt;3. ROBUST SPECTRAL ENSEMBLE CLUSTERING&lt;/h2&gt;
&lt;p&gt;Let X = {x1, x2, ··· , xn} be a set of n data points independently sampled from K clusters, represented as C = {C1 , · · · , Ck }.&lt;/p&gt;
&lt;p&gt;Denote Π = {π1, π2, ··· , πr} as r input basic partitions (BPs), each of which divides X into Ki crisply partitions and maps X into a label set πi = {πi(x1), πi(x2), · · · , πi(xn)}, where Ki is the cluster number for the i-th BP, 1 ≤ πi(xj) ≤ Ki, and 1 ≤ i ≤ r, 1 ≤ j ≤ n.&lt;/p&gt;
&lt;p&gt;Note that, the cluster number of each BP is usually set to be different for achieving the diversity among input BPs, which has been recognized as an efficient manner to ensure the success for ensemble clustering [1, 32].&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Agree&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The goal of ensemble clustering is to find the consensus parti- tion that agrees with input BPs most and divides X into its origi- nal K clusters. Commonly, EC methods may summarize r BPs as a co-association matrix, and then conduct a graph partition algo- rithm to obtain the final consensus clustering, denoted as π. The co-association matrix S ∈ Rn×n actually calculates the times of two instances occurring in the same cluster based on Π, which is defined as [8]:&lt;/p&gt;
&lt;div&gt;
$$
\mathbf{S}\left(x_{p}, x_{q}\right)=\sum_{i=1}^{r} \delta\left(\pi_{i}\left(x_{p}\right), \pi_{i}\left(x_{q}\right)\right)
$$
&lt;/div&gt;
&lt;p&gt;where $x_{p}, x_{q} \in \mathcal{X}$ and δ(a, b) is 1 if a = b; 0 otherwise. Obvi- ously, S could be normalized by S = S/r. Inspired by [16], we apply the spectral clustering on the co-association matrix S, and have its trace minimization form by following [19] as&lt;/p&gt;
&lt;div&gt;
$$
\min _{\mathbf{H}} \operatorname{tr}\left(\mathbf{H}^{\mathrm{T}} \mathbf{L}_{s} \mathbf{H}\right) \text { s.t.} \mathbf{H}^{\mathrm{T}} \mathbf{H}=\mathbf{I}, 
$$
&lt;/div&gt;
&lt;div&gt;
$$
\mathbf{L}_{s}=\mathbf{I}-\mathbf{D}_{s}^{-1 / 2} \mathbf{S} \mathbf{D}_{s}^{-1 / 2}
$$
&lt;/div&gt;
&lt;p&gt;of S, with degree matrix $D_s$ ∈ Rn×n being a diagonal matrix whose jth diagonal element is the sum of the jth row of S, and H ∈ Rn×K is defined as the scaled partition matrix of π:&lt;/p&gt;
&lt;div&gt;
$$
\mathbf{H}_{j k}=\left\{\begin{array}{ll}{1 / \sqrt{\left|C_{k}\right|}, } &amp; {\text { if } x_{j} \in C_{k} \text { in } \pi} \\ {0, } &amp; {\text { otherwise }}\end{array}\right.
$$
&lt;/div&gt;
&lt;p&gt;We can use H to represent the final ensemble clustering result.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Really? interesting, this is first time I read this.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-problem-formulation&#34;&gt;3.2 Problem Formulation&lt;/h3&gt;
&lt;p&gt;Given a normalized co-association matrix S, the objective func- tion of our RSEC is formulated as:&lt;/p&gt;
&lt;div&gt;
$$
\begin{array}{l}{\min _{\mathbf{H}, \mathbf{z}, \mathbf{E}} \operatorname{tr}\left(\mathbf{H}^{\mathrm{T}} \mathbf{L}_{z} \mathbf{H}\right)+\lambda_{1}\|\mathbf{Z}\|_{*}+\lambda_{2}\|\mathbf{E}\|_{2, 1}} \\ {\text { s.t.} \mathbf{H}^{\mathrm{T}} \mathbf{H}=\mathbf{I}, \mathbf{S}=\mathbf{S} \mathbf{Z}+\mathbf{E}}\end{array}
$$
&lt;div&gt;
with
$$
\mathbf{L}_{z}=\mathbf{I}-\mathbf{D}_{z}^{-1 / 2}\left(\left(\mathbf{Z}+\mathbf{Z}^{\mathrm{T}}\right) / 2+\mathbf{H} \mathbf{H}^{\mathrm{T}}\right) \mathbf{D}_{z}^{-1 / 2}
$$
where \(\mathbf{H}\) denotes the consensus partition, \(\mathbf{Z} \in \mathbb{R}^{n \times n}\) is the learned
representation, \(\mathbf{E} \in \mathbb{R}^{n \times n}\) is an error matrix that tries to capture
various noises inside \(\mathbf{S}, \) and \(\lambda_{1}, \lambda_{2}&gt;0\) are two penalty parameters
to balance the corresponding terms. In Eq.(5), \(\mathbf{L}_{z}\) is designed as a
normalized Laplacian matrix of the graph constructed by \(\mathbf{Z}\) and \(\mathbf{H}, \)
and the degree matrix \(\mathbf{D}_{z}\) is computed by:
&lt;/div&gt;
&lt;div&gt;
$$
\mathbf{L}_{z}=\mathbf{I}-\mathbf{D}_{z}^{-1 / 2}\left(\left(\mathbf{Z}+\mathbf{Z}^{\mathrm{T}}\right) / 2+\mathbf{H} \mathbf{H}^{\mathrm{T}}\right) \mathbf{D}_{z}^{-1 / 2}
$$
&lt;/div&gt;
&lt;div&gt;
where \(d_{j}, 1 \leq j \leq n, \) is the sum of the \(j\) -th row of the matrix
\(\left(\mathbf{Z}+\mathbf{Z}^{\mathrm{T}}\right) / 2+\mathbf{H} \mathbf{H}^{\mathrm{T}}\). Here, we employ \(\left(\mathbf{Z}+\mathbf{Z}^{\mathrm{T}}\right) / 2\) instead of \(\mathbf{Z}\)
to achieve a symmetric graph. Moreover, since \(\mathbf{H}\) is a high-quality
clustering result, \(\mathbf{H} \mathbf{H}^{\mathrm{T}}\) enjoys a clear cluster structure. Thus, we
use it to further enhance the block-diagonal structure of \(\mathbf{Z}\).
&lt;/div&gt;
&lt;h3 id=&#34;33-optimization--the-augmented-lagrange-multiplier-alm&#34;&gt;3.3 Optimization : The Augmented Lagrange Multiplier (ALM)&lt;/h3&gt;
&lt;p&gt;The Augmented Lagrange Multiplier (ALM) method [13] comes to
mind as an efficient and effective solver to our problem. To apply
ALM, we first introduce an auxiliary variable J to make Eq.(5)
separable, and equivalently convert it as:&lt;/p&gt;
&lt;div&gt;
$$
\begin{array}{ll}{} &amp; {\min _{\mathbf{H}, \mathbf{Z}, \mathbf{E}} \operatorname{tr}\left(\mathbf{H}^{\mathrm{T}} \mathbf{L}_{z} \mathbf{H}\right)+\lambda_{1}\|\mathbf{J}\|_{*}+\lambda_{2}\|\mathbf{E}\|_{2, 1}} \\ {} &amp; {\text { s.t.} \mathbf{H}^{\mathrm{T}} \mathbf{H}=\mathbf{I}, \mathbf{S}=\mathbf{S} \mathbf{Z}+\mathbf{E}, \mathbf{Z}=\mathbf{J}}\end{array}
$$
&lt;/div&gt;
&lt;div&gt;
Following \([5], \) the constraint \(\mathbf{H}^{\mathrm{T}} \mathbf{H}=\mathbf{I}\) is relaxed to avoid hard
partition during the optimization process. Then, the augmented La-
grangian function of Eq.( 8) is:
&lt;/div&gt;
&lt;div&gt;
\(\begin{aligned} \mathcal{L}=&amp; \operatorname{tr}\left(\mathbf{H}^{\mathrm{T}} \mathbf{L}_{z} \mathbf{H}\right)+\lambda_{1}\|\mathbf{J}\|_{*}+\lambda_{2}\|\mathbf{E}\|_{2, 1} \\ &amp;+\left\langle\mathbf{Y}_{1}, \mathbf{S}-\mathbf{S} \mathbf{Z}-\mathbf{E}\right\rangle+\left\langle\mathbf{Y}_{2}, \mathbf{Z}-\mathbf{J}\right\rangle \\ &amp;+\frac{\mu}{2}\left(\|\mathbf{S}-\mathbf{S} \mathbf{Z}-\mathbf{E}\|_{\mathrm{F}}^{2}+\|\mathbf{Z}-\mathbf{J}\|_{\mathrm{F}}^{2}\right) \end{aligned}\)
&lt;/div&gt;
&lt;p&gt;where Y1 and Y1 are two Lagrangian multipliers, and μ &amp;gt; 0 is the penalty parameter.&lt;/p&gt;
&lt;p&gt;The ALM solver solves Eq.(9) with an iterative update manner, which addresses J, Z, E, and H in sequence and optimizes one variable at a time by fixing the others. More details are given in the following.&lt;/p&gt;
&lt;h4 id=&#34;updata-j&#34;&gt;Updata J.&lt;/h4&gt;
&lt;div&gt;
&lt;p&gt;We first minimize (\mathcal{L}) with respect to (\mathrm{J}, ) and obtain (\mathrm{J}^{(t+1)})
by:
$$
\begin{aligned} \quad &amp;amp; \text { argmin } \lambda_{1}|\mathbf{J}|_{*}+\left\langle\mathbf{Y}_{2}^{(t)}, \mathbf{Z}^{(t)}-\mathbf{J}\right\rangle+\frac{\mu^{(t)}}{2}\left|\mathbf{Z}^{(t)}-\mathbf{J}\right|_{\mathrm{F}}^{2} \&lt;br&gt;
= \underset{\mathbf{J}}{\operatorname{argmin}} \frac{\lambda_{1}}{\mu^{(t)}}|\mathbf{J}|_{*}+\frac{1}{2}\left|\mathbf{J}-\left(\mathbf{Z}^{(t)}+\frac{1}{\mu^{(t)}} \mathbf{Y}_{2}^{(t)}\right)\right|_{\mathrm{F}}^{2} \end{aligned}
$$&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Eq.(10) could be solved by the Singular Value Thresholding (SVT) operator [3], which has a closed-form solution as:&lt;/p&gt;
&lt;div&gt;
$$
\begin{array}{c}{\mathbf{J}^{(t+1)}=\Theta_{\frac{\lambda_{1}}{\mu^{(t)}}}\left(\mathbf{Z}^{(t)}+\frac{1}{\mu^{(t)}} \mathbf{Y}_{2}^{(t)}\right)} \\ {\text { where } \Theta(\cdot) \text { is the SVT operator.}}\end{array}
$$
&lt;/div&gt;
&lt;h4 id=&#34;update-z&#34;&gt;Update Z&lt;/h4&gt;
&lt;div&gt;
Update \(\mathbf{Z} .\) By substituting Eq.(6) into \(\mathcal{L}\) and dropping unrelated
terms, the subproblem for updating \(\mathbf{Z}\) is equivalent to the following: \\
argmin \(-\frac{1}{2} \operatorname{tr}\left(\mathbf{H}^{(t) T} \mathbf{D}_{z}^{-1 / 2}\left(\mathbf{Z}+\mathbf{Z}^{\mathrm{T}}\right) \mathbf{D}_{z}^{-1 / 2} \mathbf{H}^{(t)}\right)\)
\(\quad+\left(\mathbf{Y}_{1}^{(t)}, \mathbf{S}-\mathbf{S} \mathbf{Z}-\mathbf{E}^{(t)}\right\rangle+\left\langle\mathbf{Y}_{2}^{(t)}, \mathbf{Z}-\mathbf{J}^{(t+1)}\right\rangle\)
\(\quad+\frac{\mu^{(t)}}{2}\left(\left\|\mathbf{S}-\mathbf{S} \mathbf{Z}-\mathbf{E}^{(t)}\right\|_{\mathbf{F}}^{2}+\left\|\mathbf{Z}-\mathbf{J}^{(t+1)}\right\|_{\mathbf{F}}^{2}\right)\)
&lt;/div&gt;
&lt;div&gt;
Note that, the derivative of \(\mathbf{D}_{z}\) with respect to \(\mathbf{Z}\) is relatively complex, which actually complicates the solution of obtaining \(\mathbf{Z}^{(t+1)} .\)
&lt;/div&gt;
&lt;div&gt;
By fixing \(D_{z}, \) Eq.( 12) becomes a quadratic problem of \(\mathbf{Z} .\) Thus, 
taking the derivative of \(\mathcal{L}\) with respect to \(\mathbf{Z}\) gives \(\mathbf{Z}^{(t+1)}\) as:
\(\mathbf{Z}^{(t+1)}=\left(\mathbf{S S}^{\mathrm{T}}+\mathbf{I}\right)^{-1}\left(\mathbf{S}^{\mathrm{T}} \mathbf{S}+\mathbf{J}^{(t+1)}-\mathbf{S}^{\mathrm{T}} \mathbf{E}^{(t)}+\right.\)
\(\left.\quad \frac{1}{\mu^{(t)}}\left(\mathbf{S}^{\mathrm{T}} \mathbf{Y}_{1}^{(t)}-\mathbf{Y}_{2}^{(t)}+\mathbf{D}_{z}^{-1 / 2} \mathbf{H}^{(t)} \mathbf{H}^{(t) \mathrm{T}} \mathbf{D}_{z}^{-1 / 2}\right)\right)\)
&lt;/div&gt;
&lt;h4 id=&#34;update-e&#34;&gt;Update E.&lt;/h4&gt;
&lt;h4 id=&#34;update-multipliers&#34;&gt;Update Multipliers&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Literature Review #1: Combining multiple clustering using evidence accumulation </title>
      <link>https://li-hongmin.github.io/post/notepaper_combining_multiple_clustering_using_evidence_accumulation/</link>
      <pubDate>Tue, 24 Dec 2019 16:04:40 +0900</pubDate>
      <guid>https://li-hongmin.github.io/post/notepaper_combining_multiple_clustering_using_evidence_accumulation/</guid>
      <description>&lt;p&gt;This excellent 
&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/1432715&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; was punished by Ana L. N. Fred and Anil K. Jain in 2005 IEEE Transactions on Pattern Analysis and Machine Intelligence.
It proposed &lt;strong&gt;An clustering ensemble is based on a voting strategy for various partitions.&lt;/strong&gt;
Outline of evidence accumulation clustering algorithm is below:
&lt;img src=&#34;1432715-table-1-source-large.gif&#34; alt=&#34;Outline of evidence accumulation clustering algorithm&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Here are literature review. All methods are old enough. I guess we can skim it.&lt;/p&gt;
&lt;details&gt;&lt;summary&gt;A list of clustering application&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;A number of application areas use clustering techniques for organizing or discovering structure in data, such as data mining [1], [2], information retrieval [3], [4], [5], image segmentation [6], and machine learning.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;&lt;summary&gt;A list of clustering methods&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;Examples of model-based techniques include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;parametric density approaches, such as mixture decomposition techniques [23], [24], [25], [26];&lt;/li&gt;
&lt;li&gt;prototype-based methods, such as central clustering [14], square-error clustering [27], K-means [28], [8], or K-medoids clustering [9];&lt;/li&gt;
&lt;li&gt;and shape fitting approaches [15], [6], [16].&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Most of the above techniques utilize an optimization procedure tuned to a particular cluster shape, or emphasize cluster compactness.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;Fisher et al. [31] proposed an optimization-based clustering algorithm, based on a pairwise clustering cost function, emphasizing cluster connectedness.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nonparametric density-based clustering methods attempt to identify high density clusters separated by low density regions [5], [32], [33].&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graph-theoretical approaches [34] have mostly been explored in &lt;strong&gt;hierarchical methods&lt;/strong&gt; that can be represented graphically as a tree or dendrogram [7], [8]. Both agglomerative [28], [35] and divisive approaches [36] (such as those based on the minimum spanning tree—MST [28]) have been proposed; different algorithms are obtained depending on the definition of similarity measures between patterns and between clusters [37]. The single-link (SL) and the complete-link (CL) hierarchical methods [7], [8] are the best known techniques in this class, emphasizing, respectively, connectedness and compactness of patterns in a cluster. Prototype-based hierarchical methods, which define similarity between clusters based on cluster representatives, such as the centroid, emphasize compactness. Variations of the prototype-based hierarchical clustering include the use of multiple prototypes per cluster, as in the CURE algorithm [38]. Other hierarchical agglomerative clustering algorithms follow a split and merge technique, the data being initially split into a large number of small clusters, merging being based on intercluster similarity; a final partition is selected among the clustering hierarchy by thresholding techniques are based or measures of cluster validity [39], [5], [40], [41], [42], [43].&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Treating the clustering problem as a graph partitioning problem, a recent approach, known as spectral clustering, applies spectral graph theory for clustering [44], [45], [46].&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;the-characteristic-of-k-means-algorithm&#34;&gt;The characteristic of K-means algorithm&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;minimizes the squared-error criteria, is one of the simplest clustering algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is computationally efficient and does not require the user to specify many parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Its major limitation, however, is the inability to identify clusters with arbitrary shapes, ultimately imposing hyperspherical shaped clusters on the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extensions of the basic K-means algorithm include: use of Mahalanobis distance to identify hyperellipsoidal clusters [28], introducing fuzzy set theory to obtain nonexclusive partitions [20], and adaptations to straight line fitting [47].&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;problem-description&#34;&gt;Problem description&lt;/h3&gt;
&lt;p&gt;While hundreds of clustering algorithms exist, it is difficult to find a single clustering algorithm that can handle all types of cluster shapes and sizes or even decide which algorithm would be the best one for a particular data set [48], [49].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1432715-fig-1-source-large.gif&#34; alt=&#34;Figure 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1 Results of clusterings using different algorithms (K-means, single-link—SL, and complete-link—CL) with different parameters. Each cluster identified is shown in a different color/pattern.(a) Input data.(b) K-means clustering, k=8.(c) Clustering with the SL method, threshold at 0.55, resulting in 27 clusters.(d) Clustering with the SL method, forcing eight clusters.(e) Clustering with the CL method, threshold at 2.6, resulting in 22 clusters.(f) Clustering with the CL method, forcing eight clusters.&lt;/em&gt;&lt;/p&gt;
&lt;details&gt;&lt;summary&gt; Related Work and why use k-means as basic partition method &lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;Inspired by the work in sensor fusion and classifier combination [50], [51], [52], a clustering combination approach has been proposed [53], [54], [55]. Fred and Jain introduce the concept of evidence accumulation clustering that maps the individual data partitions in a clustering ensemble into a new similarity measure between patterns, summarizing interpattern structure perceived from these clusterings. The final data partition is obtained by applying the single-link method to this new similarity matrix. The results of this method show that, the combination of “weak” clustering algorithms such as &lt;strong&gt;the K-means, which impose a simple structure on the data, can lead to the identification of true underlying clusters with arbitrary shapes, sizes and densities.&lt;/strong&gt; Strehl and Ghosh [56] explore the concept of consensus between data partitions and propose three different combination mechanisms. The first step of the consensus functions is to transform the data partitions into a hypergraph representation. The hypergraph-partitioning algorithm (HGPA) obtains the combined partition by partitioning the hypergraph into k unconnected components of approximately the same size, by cutting a minimum number of hyperedges. The metaclustering algorithm (MCLA) applies a graph-based clustering to hyperedges in the hypergraph representation. Finally, CSPA uses a pairwise similarity, as defined by Fred and Jain [55], and the final data partition is obtained by applying the &lt;strong&gt;METIS&lt;/strong&gt; algorithm of Karypis and Kumar to the induced similarity measure between patterns.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;2-problem-formulation&#34;&gt;2. Problem Formulation&lt;/h2&gt;
&lt;p&gt;Consider $N$ partitions of the data $X$ and let $(\mathrm{F})$ represent the set of $N$ partitions, which we
define as a clustering ensemble:&lt;/p&gt;
&lt;div&gt;$$\mathrm{P}=\left\{P^{1}, P^{2}, \ldots, P^{N}\right\}$$&lt;/div&gt;
&lt;div&gt;
$$
\begin{aligned} P^{1} &amp;=\left\{C_{1}^{1}, C_{2}^{1}, \ldots, C_{k_{1}}^{1}\right\} \\ &amp; \vdots \\ P^{N} &amp;=\left\{C_{1}^{N}, C_{2}^{N}, \ldots, C_{k_{N}}^{N}\right\} \end{aligned}
$$
&lt;/div&gt;
&lt;p&gt;where $C_{j}^{i}$ is the $j$ th cluster in data partition $P^{i},$ which has $k_{i}$ clusters and $n_{j}^{i}$ is the
cardinality of $C_{j}^{i},$ with $\sum_{j=1}^{k_{i}} n_{j}^{i}=n, \quad i=1, \ldots, N$&lt;/p&gt;
&lt;p&gt;The problem is to find an &amp;ldquo;optimal&amp;rdquo; data partition, $P^{&lt;em&gt;},$ using the information available in
$N$ different data partitions in $\mathrm{FP}=\left{P^{1}, P^{2}, \ldots, P^{N}\right} .$ We define $k^{&lt;/em&gt;}$ as the number of
clusters in $P^{&lt;em&gt;} .$ Ideally, $P^{&lt;/em&gt;}$ should satisfy the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Consistency with the clustering ensemble $\mathrm{F}$;&lt;/li&gt;
&lt;li&gt;Robustness to small variations in $\mathrm{F}$;&lt;/li&gt;
&lt;li&gt;Goodness of fit with ground truth information (true cluster labels of patterns), if available.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-evidence-accumulation-clustering&#34;&gt;3. Evidence accumulation clustering&lt;/h2&gt;
&lt;h3 id=&#34;31-producing-clustering-ensembles&#34;&gt;3.1 Producing Clustering Ensembles&lt;/h3&gt;
&lt;p&gt;Clustering ensembles can be generated by following two approaches:&lt;/p&gt;
&lt;details&gt;&lt;summary&gt;1) choice of data representation&lt;/summary&gt;
&lt;p&gt;In the first approach, different partitions of the objects under analysis may be produced by:&lt;/p&gt;
&lt;p&gt;a) employing different preprocessing and/or feature extraction mechanisms, which ultimately lead to different pattern representations (vectors, strings, graphs, etc.) or different feature spaces&lt;/p&gt;
&lt;p&gt;b) exploring subspaces of the same data representation, such as using subsets of features&lt;/p&gt;
&lt;p&gt;c) perturbing the data, such as in bootstrapping techniques (like bagging), or sampling approaches, as, for instance, using a set of prototype samples to represent huge data sets.&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;&lt;summary&gt;2) choice of clustering algorithms or algorithmic parameters. &lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;In the second approach, we can generate clustering ensembles by:&lt;/p&gt;
&lt;p&gt;i) applying different clustering algorithms,&lt;/p&gt;
&lt;p&gt;ii) using the same clustering algorithm with different parameters or initializations,&lt;/p&gt;
&lt;p&gt;iii) exploring different dissimilarity measures for evaluating interpattern relationships, within a given clustering algorithm.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;h3 id=&#34;32-combining-evidence-the-co-association-matrix&#34;&gt;3.2 Combining Evidence: The Co-Association Matrix&lt;/h3&gt;
&lt;p&gt;Taking the co-occurrences of pairs of patterns in the same cluster as votes for their
association, the $N$ data partitions of $n$ patterns are mapped into a $n \times n$ co-association
matrix:
$$
\mathcal{C}(i, j)=\frac{n_{i j}}{N}
$$&lt;/p&gt;
&lt;p&gt;where $n_{i j}$ is the number of times the pattern pair $(i, j)$ is assigned to the same cluster
among the $N$ partitions.&lt;/p&gt;
&lt;p&gt;Authors give an illustration of proposed methods in figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1432715-fig-2-source-large.gif&#34; alt=&#34;figure2&#34;&gt;
&lt;em&gt;Fig. 2. Individual clusterings and combination results on concentric clusters using the K-means algorithm. (a) Data set with concentric clusters. (b) First run of K-means, k=25. (c) Second run of K-means, k=11. (d) Plot of the interpattern similarity matrix for the data in (a). (e) Co-association matrix for the clustering in (b). (f) Co-association matrix for the clustering in (c). (g) Co-association matrix based on the combination of 30 clusterings. (h) Two-dimensional multidimensional scaling of the co-association matrix in (g). (i) Evidence accumulation data partition.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-recovering-natural-clusters&#34;&gt;3.3 Recovering Natural Clusters&lt;/h3&gt;
&lt;p&gt;The core of the evidence accumulation clustering technique is the mapping of partitions into the co-association matrix, $C$.
As figure shows, the clustering is obtained using the a MST-based clustering with co-association matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;1432715-fig-3-source-large.gif&#34; alt=&#34;figure3&#34;&gt;
&lt;em&gt;Fig. 3. Dendrogram produced by the SL method using the similarity matrix in Fig. 2g. Distances (1−similarity) are represented along the graph ordinate. From the dendrogram, the following cluster lifetimes are identified: 2-clusters: l2=0.18, 3-clusters: l3=0.36, 4-clusters: l4=0.14, 5-clusters: 0.02. The 3-cluster partition (shown in Fig. 2i), corresponding to the longest lifetime, is chosen (threshold on the dendrogram is between 0.4 and 0.76).&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>【学习笔记】Hugo 建站</title>
      <link>https://li-hongmin.github.io/project/hugo/hugo/</link>
      <pubDate>Mon, 23 Dec 2019 01:13:10 +0900</pubDate>
      <guid>https://li-hongmin.github.io/project/hugo/hugo/</guid>
      <description>&lt;h1 id=&#34;学习笔记集合&#34;&gt;学习笔记集合：&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;介绍了简单的Hugo应用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;跳转到
&lt;a href=&#34;https://li-hongmin.github.io/post/191217_hugo/&#34;&gt;Day1&lt;/a&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;介绍了github pages和部署。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;跳转到
&lt;a href=&#34;https://li-hongmin.github.io/post/191222_hugo/&#34;&gt;Day2&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>【学习笔记】用Hugo建站来写个人博客 Day2</title>
      <link>https://li-hongmin.github.io/post/191222_hugo/</link>
      <pubDate>Sun, 22 Dec 2019 23:53:20 +0900</pubDate>
      <guid>https://li-hongmin.github.io/post/191222_hugo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;上次我们已经做个一个网站在本地了，这次要把它部署到Github pages上。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本文主要参考
&lt;a href=&#34;https://gohugo.io/hosting-and-deployment/hosting-on-github/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hugo官方文档&lt;/a&gt;,需要github和本地两别操作。&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;部署前的准备&#34;&gt;部署前的准备&lt;/h1&gt;
&lt;p&gt;首先要确认三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确认本地有无
&lt;a href=&#34;https://git-scm.com/downloads&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git&lt;/a&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;git --version
# git version 2.21.0 (Apple Git-122.2)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;确认
&lt;a href=&#34;https://github.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;账号有无，可以免费申请一个。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;已经有一个可以发布的Hugo网站&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;两种github-pages&#34;&gt;两种GitHub Pages&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;User/Organization Pages (https://&amp;lt;USERNAME|ORGANIZATION&amp;gt;.github.io/)&lt;/li&gt;
&lt;li&gt;Project Pages (https://&amp;lt;USERNAME|ORGANIZATION&amp;gt;.github.io/&lt;PROJECT&gt;/)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;部署设置&#34;&gt;部署设置&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;在github创建新的pages repository，如果是第一种就把&amp;lt;USERNAME|ORGANIZATION&amp;gt;换成你的用户名或者组织名，则输入如下图：















&lt;figure id=&#34;figure-我因为已经有同名的repository了所以不可以创建&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;creatpages.png&#34; data-caption=&#34;我因为已经有同名的repository了，所以不可以创建。&#34;&gt;


  &lt;img src=&#34;creatpages.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    我因为已经有同名的repository了，所以不可以创建。
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再用同样的方法在github创建新的repository用来放hugo的文件，以下用&lt;code&gt;&amp;lt;YOUR-PROJECT&amp;gt;&lt;/code&gt;代替这个repository的名字&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本地git clone，把&lt;code&gt;&amp;lt;YOUR-PROJECT-URL&amp;gt;&lt;/code&gt;换成网页url地址，在合适的目录下执行命令:
&lt;code&gt;git clone &amp;lt;YOUR-PROJECT-URL&amp;gt; &amp;amp;&amp;amp; cd &amp;lt;YOUR-PROJECT&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;复制粘贴之前hugo的文件到clone的目录下面。用&lt;code&gt;hugo server&lt;/code&gt;来检查下正不正常，浏览器打开
&lt;a href=&#34;http://localhost:1313&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://localhost:1313&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;测试过后，用&lt;code&gt;rm -rf public/&lt;/code&gt;来移除所有public下的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建一个submodule：&lt;code&gt;git submodule add -b master git@github.com:&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io.git public&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这个submodule就会同步到&lt;code&gt;&amp;lt;USERNAME&amp;gt;/&amp;lt;USERNAME&amp;gt;.github.io&lt;/code&gt;，而整个本地目录会同步到&lt;code&gt;&amp;lt;YOUR-PROJECT&amp;gt;&lt;/code&gt;这里。&lt;/p&gt;
&lt;h1 id=&#34;部署网站&#34;&gt;部署网站&lt;/h1&gt;
&lt;h2 id=&#34;生成网站并部署&#34;&gt;生成网站并部署&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;hugo
cd public
git add .
git commit -m &amp;quot;Build website&amp;quot;
git push origin master
cd ..
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样应就可以了，浏览器打开&lt;code&gt;&amp;lt;USERNAME&amp;gt;.github.io&lt;/code&gt;就可以看到了。&lt;/p&gt;
&lt;h2 id=&#34;自动部署脚本&#34;&gt;自动部署脚本&lt;/h2&gt;
&lt;p&gt;可以创建一个deploy.sh文件，好像
&lt;a href=&#34;https://github.com/Li-Hongmin/academic-kickstart/blob/master/deploy.sh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这样&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;然后记得&lt;code&gt;chmod +x deploy.sh&lt;/code&gt;赋予权限。&lt;/p&gt;
&lt;p&gt;只要运行 &lt;code&gt;./deploy.sh &amp;quot;Your optional commit message&amp;quot;&lt;/code&gt;就可以提交更改了。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;流水线作业，总是不够详细。&lt;/p&gt;
&lt;p&gt;另外本网站的制作是根据academic模版，依照其
&lt;a href=&#34;https://sourcethemes.com/academic/zh/docs/deployment/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官网&lt;/a&gt;而部署，也很简单。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>【学习笔记】用Hugo建站来写个人博客 Day1</title>
      <link>https://li-hongmin.github.io/post/191217_hugo/</link>
      <pubDate>Sun, 22 Dec 2019 23:38:25 +0900</pubDate>
      <guid>https://li-hongmin.github.io/post/191217_hugo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;建站一直是一个很复杂的工程，不过总有捷径。本文介绍一种快速建立静态网站的工具hugo。hugo是基于go语言新兴静态网站生成工具，非常快速轻便。我们只需要用markdown来写博客直接生成网页，可以让我们专注于内容。另外有超多主题可供选择，换个装潢模版也很容易。&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;缘起&#34;&gt;缘起&lt;/h1&gt;
&lt;p&gt;我目前在读Ph.D平时读论文时会做一些笔记，一直想把这些笔记整理分享出来。
我于是开始尝试使用个人博客来记录学习过程整理思路，之后还能做一些页面展示研究成果。&lt;/p&gt;
&lt;p&gt;通过一番考察我选择先用
&lt;a href=&#34;gohugo.io&#34;&gt;Hugo&lt;/a&gt;建立一个网站。
详细Hugo是什么请读者自行了解，我理解就是go语言写的快捷建站工具，并且只要用markdown语言就可以写页面了。
同时在过程中把自己的心得分享出来供后来者参考。&lt;/p&gt;
&lt;h1 id=&#34;安装和一键建站&#34;&gt;安装和一键建站&lt;/h1&gt;
&lt;p&gt;这里我主要参考官网的
&lt;a href=&#34;https://gohugo.io/getting-started/quick-start/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quick Start&lt;/a&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安装 Hugo
我的环境时macOS，使用Homebrew安装：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;brew install hugo
hugo version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后得到这样的信息：Hugo Static Site Generator v0.61.0/extended darwin/amd64 BuildDate: unknown。&lt;/p&gt;
&lt;p&gt;另外说Homebrew是一个非常好的软件管理软件，一行代码就安装好了，如果还没有试过的人一定要尝试一下。&lt;/p&gt;
&lt;p&gt;如果是Windows或linux用户的话参考
&lt;a href=&#34;https://gohugo.io/getting-started/installing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;进行安装。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;一键建站&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在terminal或命令行里找个合适的文件路径，执行以下命令。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;hugo new site yourFolderName
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这时Hugo会生成一个网站模版。命令中_yourFolderName_就是文件夹名，可以随便起。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;添加主题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;主题是Hugo的一大优势，海量主题任你选。这里是
&lt;a href=&#34;https://themes.gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;主题库&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我们这里就用官方教程里的Ananke。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;cd quickstart

# Download the theme
git init
git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke
# Note for non-git users:
#   - If you do not have git installed, you can download the archive of the latest
#     version of this theme from:
#       https://github.com/budparr/gohugo-theme-ananke/archive/master.zip
#   - Extract that .zip file to get a &amp;quot;gohugo-theme-ananke-master&amp;quot; directory.
#   - Rename that directory to &amp;quot;ananke&amp;quot;, and move it into the &amp;quot;themes/&amp;quot; directory.
# End of note for non-git users.

# Edit your config.toml configuration file
# and add the Ananke theme.
echo &#39;theme = &amp;quot;ananke&amp;quot;&#39; &amp;gt;&amp;gt; config.toml
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;添加页面&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;生成第一个页面。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;hugo new posts/my-first-post.md
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一个页面看起来像这样：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;My First Post&amp;quot;
date: 2019-03-26T08:47:11+01:00
draft: true
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以用你喜欢的IDE来写markdown文件，直接就可以生成网站了。&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;运行Hugo服务器&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;终于到来一键生成网站的时候了，虽然这里的一键是一行命令。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;hugo server -D
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后提示&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Web Server is available at http://localhost:1313/ (bind address 127.0.0.1)
Press Ctrl+C to stop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;浏览器打开
&lt;a href=&#34;http://localhost:1313/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://localhost:1313/&lt;/a&gt;看看是不是成功了。&lt;/p&gt;
&lt;p&gt;看上去就像是
&lt;a href=&#34;https://li-hongmin.github.io/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这样子&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&#34;总结&#34;&gt;总结&lt;/h1&gt;
&lt;p&gt;简单易学，一行命令完事。&lt;/p&gt;
&lt;p&gt;建站大坑，容我慢慢来填。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ensemble Feature Learning to Identify Risk Factors for Predicting Secondary Cancer</title>
      <link>https://li-hongmin.github.io/publication/ensemble_feature_learning_to_identify_risk_factors_for_predicting_secondary_cancer/</link>
      <pubDate>Sun, 22 Dec 2019 18:43:03 +0900</pubDate>
      <guid>https://li-hongmin.github.io/publication/ensemble_feature_learning_to_identify_risk_factors_for_predicting_secondary_cancer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[IJCAI 2019] Distributed collaborative feature selection based on intermediate representation</title>
      <link>https://li-hongmin.github.io/publication/distributed_collaborative_feature_selection_based_on_intermediate_representation/</link>
      <pubDate>Sat, 21 Dec 2019 12:17:54 +0900</pubDate>
      <guid>https://li-hongmin.github.io/publication/distributed_collaborative_feature_selection_based_on_intermediate_representation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Large Scale Spectral Clustering Using Sparse Representation Based on Hubness</title>
      <link>https://li-hongmin.github.io/publication/lsch/</link>
      <pubDate>Fri, 20 Dec 2019 22:50:24 +0900</pubDate>
      <guid>https://li-hongmin.github.io/publication/lsch/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
